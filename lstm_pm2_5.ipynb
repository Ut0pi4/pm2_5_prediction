{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "\n",
    "from pdb import set_trace\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SOURCE_URL = \"https://cloud.tsinghua.edu.cn/d/f519a587a6d943fa9aa0/files/?p=%2F%E5%8C%97%E4%BA%AC%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F.zip&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVED_CSV = {}\n",
    "REMOVED_CSV[\"2014\"] = {\"beijing_all_20141231.csv\", \"beijing_extra_20141231.csv\"}\n",
    "REMOVED_CSV[\"2015\"] = set()\n",
    "REMOVED_CSV[\"2016\"] = set()\n",
    "REMOVED_CSV[\"2017\"] = set()\n",
    "REMOVED_CSV[\"2018\"] = set()\n",
    "REMOVED_CSV[\"2019\"] = set()\n",
    "REMOVED_CSV[\"2020\"] = set()\n",
    "\n",
    "def maybe_download(filename, work_directory):\n",
    "    \"\"\"Download the data from website, unless it's already here.\"\"\"\n",
    "    if not tf.io.gfile.exists(work_directory):\n",
    "        tf.io.gfile.makedirs(work_directory)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    # set_trace()\n",
    "    if not tf.io.gfile.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath)\n",
    "    with tf.io.gfile.GFile(filepath) as f:\n",
    "        print('Successfully downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "def extract_files(filepath):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    # filepath = Path(filepath)\n",
    "    print('Extracting', filepath)\n",
    "    #if not tf.io.gfile.exists(filepath):\n",
    "    #    tf.io.gfile.makedirs(filepath)\n",
    "        \n",
    "    with zipfile.ZipFile(filepath+\".zip\", 'r') as zip_ref:\n",
    "        #zip_ref.extractall(\"../air_quality/\")\n",
    "        \n",
    "        for member in zip_ref.infolist():\n",
    "            # set_trace()\n",
    "            \n",
    "            member.filename = member.filename.encode(\"cp437\").decode(\"utf8\")\n",
    "\n",
    "            zip_ref.extract(member, \"../air_quality/\")\n",
    "#             set_trace()\n",
    "# #             if member.filename == \"北京空气质量\"：\n",
    "# #                 os.rename(member.filename, \"pm_2_5_data\")\n",
    "#             set_trace()\n",
    "    #os.chdir(filepath) # change directory from working dir to dir with files\n",
    "\n",
    "    for item in os.listdir(filepath): # loop through items in dir\n",
    "        #set_trace()\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            #file_name = os.path.abspath(item) # get full path of files\n",
    "            zipfile_path = os.path.join(filepath, item)\n",
    "            zip_ref = zipfile.ZipFile(zipfile_path) # create zipfile object\n",
    "            zip_ref.extractall(filepath) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name) # delete zipped file\n",
    "\n",
    "def download_and_extract():\n",
    "    dest = \"../air_quality\"\n",
    "    file_name = \"北京空气质量.zip\"\n",
    "    filepath = maybe_download(file_name, dest)\n",
    "    filepath = filepath[:-4]\n",
    "    extract_files(filepath)\n",
    "\n",
    "\n",
    "def one_hot(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    N, d = data.shape\n",
    "    one_hot_targets = np.zeros((N, d, 6)) \n",
    "    for i, row in enumerate(data_new):\n",
    "        one_hot_targets[i, :, :] = np.eye(6)[row.reshape(-1)]\n",
    "#     set_trace()\n",
    "    \n",
    "    return one_hot_targets\n",
    "\n",
    "def process_data(data):\n",
    "#     data = list(map(int, data))\n",
    "    N, d = data.shape\n",
    "    \n",
    "    new_data = np.zeros((N, d))\n",
    "#     new_data = []\n",
    "   \n",
    "    m = []\n",
    "    for j, row in enumerate(data):\n",
    "        row = [float(i) if i != \"\" else 0 for i in row]\n",
    "        row = np.array(row)\n",
    "       \n",
    "        if np.any(row==0):\n",
    "#             set_trace()\n",
    "            index = np.where(row==0)\n",
    "#             set_trace()\n",
    "            if len(index[0]) == len(row):\n",
    "                m.append(j)\n",
    "                continue\n",
    "            #for ind in index[0]:\n",
    "                \n",
    "            mu = np.sum(row)/(len(row)-len(index[0]))\n",
    "            row[index[0]] = mu\n",
    "#             set_trace()\n",
    "            if mu <= 0:\n",
    "                set_trace()\n",
    "#         set_trace()\n",
    "        new_data[j, :] = row\n",
    "        \n",
    "        \n",
    "    if m:\n",
    "        for ind in m:    \n",
    "            new_data[ind] = np.sum(new_data, axis=0)/(len(new_data)-len(m))\n",
    "    new_data = one_hot(new_data)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "def check_data(row, item, year):\n",
    "    check = [True if i==\"\" else False for i in row[3:]]\n",
    "    check_true = True\n",
    "    if np.all(np.array(check)):\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "           \n",
    "            check_true = False\n",
    "    return check_true\n",
    "\n",
    "def add_removed(item, year):\n",
    "#     REMOVED_CSV[year].add(item)\n",
    "    split = item.split(\"_\")\n",
    "    split[-2] = \"extra\"\n",
    "    item_1 = \"_\"\n",
    "    item_1 = item_1.join(split)\n",
    "    REMOVED_CSV[year].add(item_1)\n",
    "    split[-2] = \"all\"\n",
    "    item_2 = \"_\"\n",
    "    item_2 = item_2.join(split)\n",
    "    REMOVED_CSV[year].add(item_2)\n",
    "#     set_trace()\n",
    "    \n",
    "\n",
    "def restructure_data(data):\n",
    "    num_doc = len(data)\n",
    "    N, d, k = data[0].shape\n",
    "    \n",
    "    new_data = np.zeros((num_doc, N, d, k))\n",
    "    \n",
    "    for i, dat in enumerate(data):\n",
    "        new_data[i, :, :, :] = dat\n",
    "    return new_data\n",
    "\n",
    "def read_folder(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "#     set_trace()\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item in REMOVED_CSV[year]:\n",
    "            continue\n",
    "        \n",
    "        filepath_csv = folderpath + \"/\" + item\n",
    "#         print(filepath_csv)\n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv, encoding=\"utf8\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    #         set_trace()\n",
    "            prev_header = []\n",
    "            for i, row in enumerate(csv_reader):\n",
    "#                 set_trace()\n",
    "                try:\n",
    "                    row[2]==\"SO2\"\n",
    "                        \n",
    "                except:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "                \n",
    "                if len(row[3:]) != 35:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "\n",
    "               \n",
    "                    \n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    if prev_header:\n",
    "                        if header != prev_header:\n",
    "                            set_trace()\n",
    "                            break\n",
    "                    prev_header = header\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "#                             REMOVED_CSV[year].add(item)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "\n",
    "        split = filepath_csv.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            if len(pm2_5) != 24:\n",
    "                add_removed(item, year)\n",
    "#                 REMOVED_CSV[year].add(item)\n",
    "                continue\n",
    "        elif split[-2] == \"extra\":\n",
    "#             set_trace()\n",
    "            if len(SO2) != 24:\n",
    "                add_removed(item, year)\n",
    "#                 REMOVED_CSV[year].add(item)\n",
    "                continue\n",
    "            if len(NO2) != 24:\n",
    "                add_removed(item, year)\n",
    "#                 REMOVED_CSV[year].add(item)\n",
    "                continue\n",
    "            if len(CO) != 24:\n",
    "                add_removed(item, year)\n",
    "#                 REMOVED_CSV[year].add(item)\n",
    "                continue\n",
    "            if len(O3) != 24:\n",
    "                add_removed(item, year)\n",
    "#                 REMOVED_CSV[year].add(item)\n",
    "                continue\n",
    "                        \n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5).T\n",
    "            pm2_5 = process_data(pm2_5)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "        if SO2:\n",
    "            SO2 = np.array(SO2).T\n",
    "            SO2 = process_data(SO2)\n",
    "            SO2s.append(SO2)\n",
    "\n",
    "        if NO2:\n",
    "            NO2 = np.array(NO2).T\n",
    "            NO2 = process_data(NO2)\n",
    "            NO2s.append(NO2)\n",
    "\n",
    "        if CO:\n",
    "            CO = np.array(CO).T\n",
    "            CO = process_data(CO)\n",
    "            COs.append(CO)\n",
    "\n",
    "        if O3:\n",
    "            O3 = np.array(O3).T\n",
    "            O3 = process_data(O3)\n",
    "            O3s.append(O3)\n",
    "    \n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "    SO2s = restructure_data(SO2s)\n",
    "    NO2s = restructure_data(NO2s)\n",
    "    COs = restructure_data(COs)\n",
    "    O3s = restructure_data(O3s)\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s, SO2s, NO2s, COs, O3s\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "#     download_and_extract()\n",
    "#     if not tf.io.gfile.exists(\"C:/Users/xiang/Desktop/Project/PR/PM2_5/air_quality/pm2_5_dataset\"):\n",
    "# #         print(tf.io.gfile.exists(\"C:/Users/xiang/Desktop/Project/PR/PM2_5/air_quality/北京空气质量/\"))\n",
    "#         try:\n",
    "#             os.rename(\"C:/Users/xiang/Desktop/Project/PR/PM2_5/air_quality/北京空气质量\", \"C:/Users/xiang/Desktop/Project/PR/PM2_5/air_quality/pm2_5_dataset\")\n",
    "#         except:\n",
    "#             print(\"unable to change name\")\n",
    "    folderpath = \"../air_quality/北京空气质量\"\n",
    "    for key, values in REMOVED_CSV.items():\n",
    "        print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "    print(\"\")  \n",
    "    \n",
    "    pm2_5s_years = []\n",
    "    SO2s_years = []\n",
    "    NO2s_years = []\n",
    "    COs_years = []\n",
    "    O3s_years = []\n",
    "    if tf.io.gfile.exists(folderpath):\n",
    "        for item in os.listdir(folderpath):\n",
    "            if \".zip\" not in item and \"beijing\" in item:\n",
    "                pm2_5s, SO2s, NO2s, COs, O3s = read_folder(folderpath+\"/\"+item)\n",
    "                \n",
    "                pm2_5s_years.append(pm2_5s)\n",
    "                SO2s_years.append(SO2s)\n",
    "                NO2s_years.append(NO2s)\n",
    "                COs_years.append(COs)\n",
    "                O3s_years.append(O3s)\n",
    "                \n",
    "                \n",
    "    \n",
    "    for key, values in REMOVED_CSV.items():\n",
    "        print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "    print(\"\")            \n",
    "    \n",
    "    print(\"Start checking REMOVED_CSV pairs\")\n",
    "    for key, values in REMOVED_CSV.items():\n",
    "        for value_a in values:\n",
    "            two = 1\n",
    "            split = value_a.split(\"_\")\n",
    "            for value_b in values:\n",
    "                if value_a != value_b:\n",
    "                    if split[-1] in value_b:\n",
    "                        two += 1\n",
    "                        break                    \n",
    "            if two != 2:\n",
    "                set_trace()\n",
    "    print(\"Complete check\")\n",
    "#     set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"seperates data into batches of size batch_size\"\"\"\n",
    "# def batchify(data, batch_size, shuffle_data=True):\n",
    "    \n",
    "#     if shuffle_data == True:\n",
    "#         shuffle(data)\n",
    "    \n",
    "#     number_of_batches = len(data) // batch_size\n",
    "#     batches = list(range(number_of_batches))\n",
    "#     longest_elements = list(range(number_of_batches))\n",
    "    \n",
    "#     for batch_number in range(number_of_batches):\n",
    "#         longest_input = 0\n",
    "#         longest_target = 0\n",
    "#         input_variables = list(range(batch_size))\n",
    "#         target_variables = list(range(batch_size))\n",
    "#         index = 0      \n",
    "#         for pair in range((batch_number*batch_size),((batch_number+1)*batch_size)):\n",
    "#             input_variables[index], target_variables[index] = tensorsFromPair(input_lang, output_lang, data[pair])\n",
    "#             if len(input_variables[index]) >= longest_input:\n",
    "#                 longest_input = len(input_variables[index])\n",
    "#             if len(target_variables[index]) >= longest_target:\n",
    "#                 longest_target = len(target_variables[index])\n",
    "#             index += 1\n",
    "#         batches[batch_number] = (input_variables, target_variables)\n",
    "#         longest_elements[batch_number] = (longest_input, longest_target)\n",
    "#     return batches , longest_elements, number_of_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model: seq2seq <br>\n",
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "# #         embedded = self.embedding(input).view(1, 1, -1)\n",
    "# #         output = embedded\n",
    "#         output = input.view(24,1,6)\n",
    "# #         set_trace()\n",
    "#         output, hidden = self.lstm(output.float(), (hn.detach(), cn.detach()))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "# #         h_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         c_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         if torch.cuda.is_available():\n",
    "# #             return h_hidden.cuda(), c_hidden.cuda()\n",
    "# #         else:\n",
    "# #             return h_hidden, c_hidden\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "        output = input.view(24, self.hidden_size)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, (hn, cn))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderAttn(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
    "#         super(DecoderAttn, self).__init__()\n",
    "\n",
    "#         if bidirectional:\n",
    "#             self.directions = 2\n",
    "#         else:\n",
    "#             self.directions = 1\n",
    "#         self.output_size = output_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = layers\n",
    "#         self.dropout = dropout\n",
    "# #         self.embedder = nn.Embedding(output_size,hidden_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.score_learner = nn.Linear(hidden_size*self.directions, \n",
    "#                                    hidden_size*self.directions)\n",
    "#         self.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "#                         num_layers=layers,dropout=dropout,\n",
    "#                         bidirectional=bidirectional,batch_first=False)\n",
    "#         self.context_combiner = nn.Linear((hidden_size*self.directions)\n",
    "#                                       +(hidden_size*self.directions), hidden_size)\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.output = nn.Linear(hidden_size, output_size)\n",
    "#         self.soft = nn.Softmax(dim=1)\n",
    "#         self.log_soft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "#     def forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
    "\n",
    "# #         embedded_data = self.embedder(input_data)\n",
    "# #         embedded_data = self.dropout(embedded_data)\n",
    "# #         batch_size = embedded_data.shape[1]\n",
    "#         hiddens, outputs = self.lstm(input_data, (h_hidden, c_hidden))\n",
    "#         top_hidden = outputs[0].view(self.num_layers,self.directions,\n",
    "#                                  hiddens.shape[1],\n",
    "#                                  self.hidden_size)[self.num_layers-1]\n",
    "#         top_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1, 1)\n",
    "\n",
    "#         prep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n",
    "#         scores = torch.bmm(prep_scores, top_hidden)\n",
    "#         attn_scores = self.soft(scores)\n",
    "#         con_mat = torch.bmm(encoder_hiddens.permute(1,2,0),attn_scores)\n",
    "#         h_tilde = self.tanh(self.context_combiner(torch.cat((con_mat,\n",
    "#                                                          top_hidden),dim=1)\n",
    "#                                               .view(batch_size,-1)))\n",
    "#         pred = self.output(h_tilde)\n",
    "#         pred = self.log_soft(pred)\n",
    "\n",
    "\n",
    "#         return pred, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    hn = encoder.initHidden()\n",
    "    cn = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (hn, cn) = encoder(\n",
    "            input_tensor[ei], hn, cn)\n",
    "        \n",
    "        encoder_outputs[ei, :, :] = encoder_output.view(24, encoder.hidden_size)\n",
    "#         set_trace()\n",
    "\n",
    "#     decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_input = encoder_outputs[-1]\n",
    "#     set_trace()\n",
    "    decoder_hidden = (hn, cn)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data, i):\n",
    "    data1 = data[:i, :, :, :]\n",
    "    data2 = data[i+1:, :, :, :]\n",
    "    new_data = np.concatenate((data1, data2), axis=0)\n",
    "    new_data = new_data.reshape((-1, 24, 6))\n",
    "    return new_data\n",
    "\n",
    "def save_checkpoint(epoch, encoder, decoder, encode_optimizer, decode_optimizer):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    :param epoch: epoch number\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'encoder': encoder,\n",
    "             'decoder': decoder,\n",
    "             'encode_optimizer': encode_optimizer,\n",
    "             'decode_optimizer': decode_optimizer}\n",
    "    filename = '../checkpoint_lstm.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def trainIters(year_data, encoder, decoder, encode_optimizer, decode_optimizer, \n",
    "               epochs, print_every=1, plot_every=1, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "#     set_trace()\n",
    "    G, b, m, k = year_data.shape\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(G):\n",
    "            input_tensor = reshape_data(year_data, i)\n",
    "            input_tensor = torch.from_numpy(input_tensor).cuda()\n",
    "            target_tensor = year_data[i, :, :, :].reshape((-1, 24, 6))\n",
    "            target_tensor = torch.from_numpy(target_tensor).cuda()\n",
    "#             set_trace()\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epochs),\n",
    "                                         epoch, epoch / epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "#     for iter in range(1, n_iters + 1):\n",
    "#         training_pair = training_pairs[iter - 1]\n",
    "#         input_tensor = training_pair[0]\n",
    "#         target_tensor = training_pair[1]\n",
    "\n",
    "#         loss = train(input_tensor, target_tensor, encoder,\n",
    "#                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#         print_loss_total += loss\n",
    "#         plot_loss_total += loss\n",
    "\n",
    "#         if iter % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "#                                          iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "#         if iter % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         output = embedded\n",
    "        output = input.view(24,1,6)\n",
    "        \n",
    "        output, hidden = self.lstm(output.float(), (hn.detach(), cn.detach()))\n",
    "#         set_trace()\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "input_size = 6\n",
    "output_size = 6\n",
    "epochs = 10\n",
    "\n",
    "checkpoint = '../checkpoint_lstm.pth.tar'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    encoder = checkpoint[\"encoder\"]\n",
    "    decoder = checkpoint[\"decoder\"]\n",
    "    epoch = checkpoint[\"epoch\"] + 1\n",
    "    encode_optimizer = checkpoint[\"encode_optimizer\"]\n",
    "    decode_optimizer = checkpoint[\"decode_optimizer\"]\n",
    "else:\n",
    "    encoder = EncoderRNN(input_size, hidden_size).cuda()\n",
    "    # attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
    "    decoder = DecoderRNN(hidden_size, output_size).cuda()\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "if epoch < epochs:\n",
    "    trainIters(pm2_5s_years[1], encoder, decoder, encode_optimizer, decode_optimizer, epochs)\n",
    "\n",
    "print(\"finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
