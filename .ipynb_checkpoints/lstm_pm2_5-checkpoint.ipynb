{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "\n",
    "from pdb import set_trace\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SOURCE_URL = \"https://cloud.tsinghua.edu.cn/d/f519a587a6d943fa9aa0/files/?p=%2F%E5%8C%97%E4%BA%AC%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F.zip&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename, work_directory):\n",
    "    \"\"\"Download the data from website, unless it's already here.\"\"\"\n",
    "    if not tf.io.gfile.exists(work_directory):\n",
    "        tf.io.gfile.makedirs(work_directory)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    # set_trace()\n",
    "    if not tf.io.gfile.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath)\n",
    "    with tf.io.gfile.GFile(filepath) as f:\n",
    "        print('Successfully downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "def extract_files(filepath):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    # filepath = Path(filepath)\n",
    "    print('Extracting', filepath)\n",
    "    #if not tf.io.gfile.exists(filepath):\n",
    "    #    tf.io.gfile.makedirs(filepath)\n",
    "        \n",
    "    with zipfile.ZipFile(filepath+\".zip\", 'r') as zip_ref:\n",
    "        #zip_ref.extractall(\"../air_quality/\")\n",
    "        \n",
    "        for member in zip_ref.infolist():\n",
    "            # set_trace()\n",
    "            \n",
    "            member.filename = member.filename.encode(\"cp437\").decode(\"utf8\")\n",
    "\n",
    "            zip_ref.extract(member, \"../air_quality/\")\n",
    "#             set_trace()\n",
    "# #             if member.filename == \"北京空气质量\"：\n",
    "# #                 os.rename(member.filename, \"pm_2_5_data\")\n",
    "#             set_trace()\n",
    "    #os.chdir(filepath) # change directory from working dir to dir with files\n",
    "\n",
    "    for item in os.listdir(filepath): # loop through items in dir\n",
    "        #set_trace()\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            #file_name = os.path.abspath(item) # get full path of files\n",
    "            zipfile_path = os.path.join(filepath, item)\n",
    "            zip_ref = zipfile.ZipFile(zipfile_path) # create zipfile object\n",
    "            zip_ref.extractall(filepath) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name) # delete zipped file\n",
    "\n",
    "def download_and_extract():\n",
    "    dest = \"../air_quality\"\n",
    "    file_name = \"北京空气质量.zip\"\n",
    "    filepath = maybe_download(file_name, dest)\n",
    "    filepath = filepath[:-4]\n",
    "    extract_files(filepath)\n",
    "# download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data_pm2_5(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "def process_data(data, pm2_5=0):\n",
    "#     data = list(map(int, data))\n",
    "    N, d = data.shape\n",
    "    \n",
    "    new_data = np.zeros((N, d))\n",
    "#     new_data = []\n",
    "   \n",
    "    m = []\n",
    "    for j, row in enumerate(data):\n",
    "        row = [float(i) if i != \"\" else 0 for i in row]\n",
    "        row = np.array(row)\n",
    "       \n",
    "        if np.any(row==0):\n",
    "#             set_trace()\n",
    "            index = np.where(row==0)\n",
    "#             set_trace()\n",
    "            if len(index[0]) == len(row):\n",
    "                m.append(j)\n",
    "                continue\n",
    "            #for ind in index[0]:\n",
    "                \n",
    "            mu = np.sum(row)/(len(row)-len(index[0]))\n",
    "            row[index[0]] = mu\n",
    "#             set_trace()\n",
    "            if mu <= 0:\n",
    "                set_trace()\n",
    "#         set_trace()\n",
    "        new_data[j, :] = row\n",
    "        \n",
    "        \n",
    "    if m:\n",
    "        for ind in m:    \n",
    "            new_data[ind] = np.sum(new_data, axis=0)/(len(new_data)-len(m))\n",
    "    \n",
    "    if pm2_5:\n",
    "        new_data = normalize_data_pm2_5(new_data)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "def check_data(row, item, year):\n",
    "    check = [True if i==\"\" else False for i in row[3:]]\n",
    "    check_true = True\n",
    "    if np.all(np.array(check)):\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "           \n",
    "            check_true = False\n",
    "    return check_true\n",
    "\n",
    "def add_removed(item, year):\n",
    "    split = item.split(\"_\")\n",
    "    split[-2] = \"extra\"\n",
    "    item_1 = \"_\"\n",
    "    item_1 = item_1.join(split)\n",
    "    REMOVED_CSV[year].add(item_1)\n",
    "    split[-2] = \"all\"\n",
    "    item_2 = \"_\"\n",
    "    item_2 = item_2.join(split)\n",
    "    REMOVED_CSV[year].add(item_2)\n",
    "    \n",
    "\n",
    "def restructure_data(data):\n",
    "    num_doc = len(data)\n",
    "\n",
    "    N, d = data[0].shape\n",
    "    \n",
    "    new_data = np.zeros((num_doc, N, d))\n",
    "    \n",
    "    for i, dat in enumerate(data):\n",
    "        new_data[i, :, :] = dat\n",
    "    return new_data\n",
    "\n",
    "def clean(folderpath, consistent=True):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        \n",
    "        split = item.split(\"_\")\n",
    "        if \"beijing\" not in item:\n",
    "            REMOVED_CSV[year].add(item)\n",
    "            continue\n",
    "#         set_trace()\n",
    "        if consistent:\n",
    "            if split[-2] == \"extra\":\n",
    "                split_1 = split\n",
    "                split_1[-2] = \"all\"\n",
    "                item_1 = \"_\"\n",
    "                item_1 = item_1.join(split_1)\n",
    "\n",
    "                if item_1 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "            else:\n",
    "                split_2 = split\n",
    "                split_2[-2] = \"extra\"\n",
    "                item_2 = \"_\"\n",
    "                item_2 = item_2.join(split_2)\n",
    "                if item_2 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "        \n",
    "        \n",
    "        if item in REMOVED_CSV[year]:\n",
    "            continue\n",
    "        \n",
    "        filepath_csv = folderpath + \"/\" + item\n",
    "#         print(filepath_csv)\n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv, encoding=\"utf8\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    #         set_trace()\n",
    "            prev_header = []\n",
    "            for i, row in enumerate(csv_reader):\n",
    "#                 set_trace()\n",
    "                try:\n",
    "                    row[2]==\"SO2\"\n",
    "                        \n",
    "                except:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "                \n",
    "                if len(row[3:]) != 35:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "\n",
    "               \n",
    "                    \n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    if prev_header:\n",
    "                        if header != prev_header:\n",
    "                            set_trace()\n",
    "                            break\n",
    "                    prev_header = header\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "#                             REMOVED_CSV[year].add(item)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "\n",
    "        split = filepath_csv.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            if len(pm2_5) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "        elif split[-2] == \"extra\":\n",
    "            if len(SO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(NO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(CO) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(O3) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "    print(\"complete read folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of REMOVED_CSV in year 2014: 2\n",
      "length of REMOVED_CSV in year 2015: 0\n",
      "length of REMOVED_CSV in year 2016: 0\n",
      "length of REMOVED_CSV in year 2017: 0\n",
      "length of REMOVED_CSV in year 2018: 0\n",
      "length of REMOVED_CSV in year 2019: 0\n",
      "length of REMOVED_CSV in year 2020: 0\n",
      "\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "length of REMOVED_CSV in year 2014: 154\n",
      "length of REMOVED_CSV in year 2015: 29\n",
      "length of REMOVED_CSV in year 2016: 64\n",
      "length of REMOVED_CSV in year 2017: 240\n",
      "length of REMOVED_CSV in year 2018: 118\n",
      "length of REMOVED_CSV in year 2019: 116\n",
      "length of REMOVED_CSV in year 2020: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REMOVED_CSV = {}\n",
    "REMOVED_CSV[\"2014\"] = {\"beijing_all_20141231.csv\", \"beijing_extra_20141231.csv\"}\n",
    "REMOVED_CSV[\"2015\"] = set()\n",
    "REMOVED_CSV[\"2016\"] = set()\n",
    "REMOVED_CSV[\"2017\"] = set()\n",
    "REMOVED_CSV[\"2018\"] = set()\n",
    "REMOVED_CSV[\"2019\"] = set()\n",
    "REMOVED_CSV[\"2020\"] = set()\n",
    "\n",
    "folderpath = \"../air_quality/北京空气质量\"\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")  \n",
    "\n",
    "\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            clean(folderpath+\"/\"+item)\n",
    "\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")            \n",
    "\n",
    "# print(\"Start checking REMOVED_CSV pairs\")\n",
    "# for key, values in REMOVED_CSV.items():\n",
    "#     for value_a in values:\n",
    "#         if \"beijing\" in value_a:\n",
    "#             two = 1\n",
    "#             split = value_a.split(\"_\")\n",
    "#             for value_b in values:\n",
    "#                 if value_a != value_b:\n",
    "#                     if split[-1] in value_b:\n",
    "#                         set_trace()\n",
    "#                         two += 1\n",
    "#                         break                    \n",
    "#             if two != 2:\n",
    "#                 set_trace()\n",
    "# print(\"Complete check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def retrieve_data_alls(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "            split = item.split(\"_\")\n",
    "            if split[-2] == \"extra\":\n",
    "                extras.append(item)\n",
    "            else:\n",
    "                alls.append(item)\n",
    "\n",
    "    alls_pairs = []\n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = alls[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "\n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            alls_pairs.append((alls[i], alls[i+1]))\n",
    "        \n",
    "\n",
    "    for (item_1, item_2) in alls_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        pm2_5 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "           \n",
    "        \n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                        \n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 1)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "    \n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s\n",
    "\n",
    "def retrieve_data_extras(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    \n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "\n",
    "    for item in items:\n",
    "        split = item.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            split_1 = split\n",
    "            split_1[-2] = \"extra\"\n",
    "            item_1 = \"_\"\n",
    "            item_1 = item_1.join(split_1)\n",
    "            if item_1 in items:\n",
    "                extras.append(item_1)\n",
    "                alls.append(item)\n",
    "                \n",
    "    assert(len(extras)==len(alls))\n",
    "    extra_all_pairs = []\n",
    "    \n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = extras[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "        \n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            extra_all_pairs.append((extras[i], alls[i+1]))\n",
    "        \n",
    "#     set_trace()\n",
    "    for (item_1, item_2) in extra_all_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "           \n",
    "        \n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "                        \n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 1)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "        if SO2:\n",
    "            SO2 = np.array(SO2)\n",
    "#             set_trace()\n",
    "            SO2 = process_data(SO2)\n",
    "#             set_trace()\n",
    "            SO2s.append(SO2)\n",
    "\n",
    "        if NO2:\n",
    "            NO2 = np.array(NO2)\n",
    "            NO2 = process_data(NO2)\n",
    "            NO2s.append(NO2)\n",
    "\n",
    "        if CO:\n",
    "            CO = np.array(CO)\n",
    "            CO = process_data(CO)\n",
    "            COs.append(CO)\n",
    "\n",
    "        if O3:\n",
    "            O3 = np.array(O3)\n",
    "            O3 = process_data(O3)\n",
    "            O3s.append(O3)\n",
    "    \n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "    SO2s = restructure_data(SO2s)\n",
    "    NO2s = restructure_data(NO2s)\n",
    "    COs = restructure_data(COs)\n",
    "    O3s = restructure_data(O3s)\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s, SO2s, NO2s, COs, O3s\n",
    "#     return pm2_5s\n",
    "\n",
    "pm2_5s_1_years = []\n",
    "pm2_5s_2_years = []\n",
    "SO2s_years = []\n",
    "NO2s_years = []\n",
    "COs_years = []\n",
    "O3s_years = []\n",
    "\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            pm2_5s_1 = retrieve_data_alls(folderpath+\"/\"+item)\n",
    "            pm2_5s_1_years.append(pm2_5s_1)\n",
    "            \n",
    "            pm2_5s_2, SO2s, NO2s, COs, O3s = retrieve_data_extras(folderpath+\"/\"+item)  \n",
    "            pm2_5s_2_years.append(pm2_5s_2)\n",
    "            SO2s_years.append(SO2s)\n",
    "            NO2s_years.append(NO2s)\n",
    "            COs_years.append(COs)\n",
    "            O3s_years.append(O3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[46.        , 46.        , 48.        , ..., 30.        ,\n",
       "         19.        , 11.        ],\n",
       "        [41.        , 40.        , 44.        , ..., 27.        ,\n",
       "         36.79411765, 31.        ],\n",
       "        [29.        , 32.        , 35.        , ..., 25.        ,\n",
       "         32.09090909,  2.        ],\n",
       "        ...,\n",
       "        [50.        , 32.        , 47.        , ..., 18.        ,\n",
       "          2.        ,  3.        ],\n",
       "        [34.        , 20.        , 26.        , ...,  3.        ,\n",
       "         24.08823529,  2.        ],\n",
       "        [30.        , 33.        , 12.        , ...,  2.        ,\n",
       "         13.        , 10.        ]],\n",
       "\n",
       "       [[44.        , 52.        , 34.        , ..., 20.        ,\n",
       "         27.        , 31.        ],\n",
       "        [45.        , 48.        , 44.        , ..., 30.        ,\n",
       "         33.78787879, 40.        ],\n",
       "        [51.        , 57.        , 55.        , ..., 39.        ,\n",
       "         28.        , 47.        ],\n",
       "        ...,\n",
       "        [78.        , 69.        , 84.        , ..., 64.        ,\n",
       "         35.        , 30.        ],\n",
       "        [63.        , 71.        , 73.        , ..., 57.        ,\n",
       "         79.70588235, 29.        ],\n",
       "        [65.        , 66.        , 62.        , ..., 39.        ,\n",
       "         37.        , 35.        ]],\n",
       "\n",
       "       [[22.        , 11.        , 10.        , ..., 10.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.        ,  7.        ,  2.        , ...,  2.        ,\n",
       "         18.12121212,  2.        ],\n",
       "        [ 3.        ,  4.        ,  2.        , ...,  2.        ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [52.        , 63.        , 86.        , ..., 33.        ,\n",
       "          8.        ,  3.        ],\n",
       "        [ 3.        , 18.        , 67.        , ..., 22.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [11.        ,  5.        , 29.        , ...,  7.        ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.5       ,  8.        ,  2.        , ...,  3.7       ,\n",
       "          2.        ,  6.7       ],\n",
       "        [ 2.7       ,  7.8       ,  2.        , ...,  2.        ,\n",
       "          2.        ,  7.        ],\n",
       "        [ 5.1       ,  7.6       ,  2.        , ...,  2.7       ,\n",
       "          2.        ,  5.2       ],\n",
       "        ...,\n",
       "        [ 2.        ,  9.1       ,  2.        , ...,  5.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  8.5       ,  2.1       , ...,  4.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  9.        ,  2.7       , ...,  4.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[10.371875  , 12.1       ,  6.6       , ...,  3.3       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 9.26875   , 11.7       ,  6.        , ...,  2.9       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 8.628125  , 11.2       ,  5.8       , ...,  2.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 6.        , 12.8       ,  7.9       , ...,  6.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.8       ,  8.2       , ...,  5.8       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.5       ,  7.5       , ...,  5.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[ 6.5       , 12.5       ,  7.9       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 13.3       ,  7.        , ...,  6.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 13.4       ,  6.2       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 5.7       , 11.2       ,  6.9       , ...,  4.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.9       , 11.5       ,  7.        , ...,  3.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 12.4       ,  7.3       , ...,  2.3       ,\n",
       "          2.        ,  2.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pm2_5s_2_years[0]\n",
    "O3s_years[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_concat(arr_data):\n",
    "    norm_arr_data = []\n",
    "    for data in arr_data:\n",
    "        norm_arr_data.append(normalize(data))\n",
    "#     set_trace()\n",
    "    return concat(norm_arr_data)\n",
    "\n",
    "def normalize(data):\n",
    "    new_data = []\n",
    "    for dat in data:\n",
    "#         set_trace()\n",
    "        temp = (dat-np.min(dat))/(np.max(dat)-np.min(dat))\n",
    "        new_data.append(temp)\n",
    "    \n",
    "#     set_trace()\n",
    "    return new_data\n",
    "\n",
    "def concat(arr_data):\n",
    "    years = len(arr_data[0])\n",
    "    new_data = []\n",
    "    for year in range(years):\n",
    "        temp = []\n",
    "        for data in arr_data:\n",
    "            temp.append(data[year])\n",
    "        \n",
    "        new_data.append(np.concatenate(temp, 0))\n",
    "#         set_trace()\n",
    "    return new_data\n",
    "\n",
    "feature_data = normalize_concat((SO2s_years, NO2s_years, O3s_years, COs_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years: 7\n",
      "data for 2015: (836, 24, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"years:\", len(feature_data))\n",
    "print(\"data for 2015:\", feature_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_years(arr_data):\n",
    "    years = len(arr_data)\n",
    "#     new_data = \n",
    "    sum_doc = 0\n",
    "    N, h, f = arr_data[0].shape\n",
    "    for year in range(years):\n",
    "#         arr_data =\n",
    "        sum_doc += arr_data[year].shape[0]\n",
    "#     set_trace()\n",
    "    new_data = np.zeros((sum_doc, h, f))\n",
    "    ind = 0\n",
    "    for year in range(years):\n",
    "        ind_last = arr_data[year].shape[0]\n",
    "#         set_trace()\n",
    "        new_data[ind:ind+ind_last, :, :] = arr_data[year]\n",
    "        ind = ind + ind_last\n",
    "#     set_trace()\n",
    "    return new_data    \n",
    "train_data = concat_years(pm2_5s_1_years[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model: seq2seq <br>\n",
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "# #         embedded = self.embedding(input).view(1, 1, -1)\n",
    "# #         output = embedded\n",
    "#         output = input.view(24,1,6)\n",
    "# #         set_trace()\n",
    "#         output, hidden = self.lstm(output.float(), (hn.detach(), cn.detach()))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "# #         h_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         c_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         if torch.cuda.is_available():\n",
    "# #             return h_hidden.cuda(), c_hidden.cuda()\n",
    "# #         else:\n",
    "# #             return h_hidden, c_hidden\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "#         output = input.view(24, self.hidden_size)\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.lstm(output, (hn, cn))\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "        \n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderAttn(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
    "#         super(DecoderAttn, self).__init__()\n",
    "\n",
    "#         if bidirectional:\n",
    "#             self.directions = 2\n",
    "#         else:\n",
    "#             self.directions = 1\n",
    "#         self.output_size = output_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = layers\n",
    "#         self.dropout = dropout\n",
    "# #         self.embedder = nn.Embedding(output_size,hidden_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.score_learner = nn.Linear(hidden_size*self.directions, \n",
    "#                                    hidden_size*self.directions)\n",
    "#         self.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "#                         num_layers=layers,dropout=dropout,\n",
    "#                         bidirectional=bidirectional,batch_first=False)\n",
    "#         self.context_combiner = nn.Linear((hidden_size*self.directions)\n",
    "#                                       +(hidden_size*self.directions), hidden_size)\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.output = nn.Linear(hidden_size, output_size)\n",
    "#         self.soft = nn.Softmax(dim=1)\n",
    "#         self.log_soft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "#     def forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
    "\n",
    "# #         embedded_data = self.embedder(input_data)\n",
    "# #         embedded_data = self.dropout(embedded_data)\n",
    "# #         batch_size = embedded_data.shape[1]\n",
    "#         hiddens, outputs = self.lstm(input_data, (h_hidden, c_hidden))\n",
    "#         top_hidden = outputs[0].view(self.num_layers,self.directions,\n",
    "#                                  hiddens.shape[1],\n",
    "#                                  self.hidden_size)[self.num_layers-1]\n",
    "#         top_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1, 1)\n",
    "\n",
    "#         prep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n",
    "#         scores = torch.bmm(prep_scores, top_hidden)\n",
    "#         attn_scores = self.soft(scores)\n",
    "#         con_mat = torch.bmm(encoder_hiddens.permute(1,2,0),attn_scores)\n",
    "#         h_tilde = self.tanh(self.context_combiner(torch.cat((con_mat,\n",
    "#                                                          top_hidden),dim=1)\n",
    "#                                               .view(batch_size,-1)))\n",
    "#         pred = self.output(h_tilde)\n",
    "#         pred = self.log_soft(pred)\n",
    "\n",
    "\n",
    "#         return pred, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    hn = encoder.initHidden()\n",
    "    cn = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (hn, cn) = encoder(\n",
    "            input_tensor[ei], hn, cn)\n",
    "        \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    decoder_input = target_tensor[0, :]\n",
    "           \n",
    "    for di in range(target_length):\n",
    "        decoder_output, (hn, cn) = decoder(decoder_input, hn, cn) \n",
    "        loss += criterion(decoder_output, target_tensor[di, :].view(-1,).long())\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di, :]\n",
    "        else:\n",
    "            decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            topv, topi = decoder_output.topk(1, dim=2)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "#         print(decoder_output.shape)\n",
    "#         print(decoder_input.shape)\n",
    "#         set_trace()\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "#     set_trace()\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data, i):\n",
    "    data1 = data[:, :, :i]\n",
    "    data2 = data[:, :, i+1:]\n",
    "    new_data = np.concatenate((data1, data2), axis=2)\n",
    "#     set_trace()\n",
    "#     new_data = new_data.reshape((-1, 24, 34))\n",
    "    return new_data\n",
    "\n",
    "def save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    :param epoch: epoch number\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'encoder': encoder.state_dict(),\n",
    "             'decoder': decoder.state_dict(),\n",
    "             'encoder_optimizer': encoder_optimizer,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = '../checkpoint_lstm.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def trainIters(year_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "               start_epoch, epochs, print_every=1, plot_every=1, learning_rate=0.01):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    g, t, f= year_data.shape\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        \n",
    "#         for i in range(f):\n",
    "#             print(\"training for pm2.5: %d\" %(i+1))\n",
    "        input_tensor = year_data[:,:24,:]\n",
    "        input_tensor = torch.from_numpy(input_tensor).to(device)\n",
    "        target_tensor = year_data[:, 24:24+6, :]\n",
    "        target_tensor = torch.from_numpy(target_tensor).to(device)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                 decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epochs),\n",
    "                                         epoch, epoch / epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training from epoch 31\n",
      "0m 12s (- 0m 7s) (31 62%) 1.7976\n",
      "0m 24s (- 0m 13s) (32 64%) 1.7977\n",
      "0m 36s (- 0m 18s) (33 66%) 1.7976\n",
      "0m 48s (- 0m 22s) (34 68%) 1.7976\n",
      "1m 0s (- 0m 25s) (35 70%) 1.8207\n",
      "1m 12s (- 0m 28s) (36 72%) 1.8206\n",
      "1m 24s (- 0m 29s) (37 74%) 1.7975\n",
      "1m 36s (- 0m 30s) (38 76%) 1.8207\n",
      "1m 47s (- 0m 30s) (39 78%) 1.8206\n",
      "1m 59s (- 0m 29s) (40 80%) 1.8206\n",
      "2m 11s (- 0m 28s) (41 82%) 1.8206\n",
      "2m 22s (- 0m 27s) (42 84%) 1.8206\n",
      "2m 34s (- 0m 25s) (43 86%) 1.7975\n",
      "2m 46s (- 0m 22s) (44 88%) 1.7976\n",
      "2m 57s (- 0m 19s) (45 90%) 1.8207\n",
      "3m 9s (- 0m 16s) (46 92%) 1.7975\n",
      "3m 21s (- 0m 12s) (47 94%) 1.7976\n",
      "3m 32s (- 0m 8s) (48 96%) 1.7976\n",
      "3m 44s (- 0m 4s) (49 98%) 1.8207\n",
      "3m 55s (- 0m 0s) (50 100%) 1.8206\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 6\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.out_1 = nn.Linear(1, 6)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "        output = input.view(self.seq_len, 1, self.input_size)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "        output = output.view(self.seq_len, self.hidden_size)\n",
    "        \n",
    "        output = self.out(output).view(self.seq_len, self.output_size, 1)\n",
    "        output = self.out_1(output)\n",
    "        output = self.softmax(output)\n",
    "        output = output.view(-1, 6)\n",
    "#         set_trace()\n",
    "#         output = output.view(self.seq_len, self.input_size)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 24\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "        \n",
    "        output = input.view(self.seq_len, 1, self.input_size)\n",
    "        output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "        \n",
    "#         set_trace()\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "input_size_enc = 35\n",
    "input_size_dec = 35\n",
    "output_size = 35\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "checkpoint = '../checkpoint_lstm.pth.tar'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "encoder = EncoderRNN(input_size_enc, hidden_size).to(device)\n",
    "decoder = DecoderRNN(input_size_dec, hidden_size, output_size).to(device)\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    checkpoint = torch.load(checkpoint)    \n",
    "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "    decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "    \n",
    "    epoch = checkpoint[\"epoch\"]+1\n",
    "    encoder_optimizer = checkpoint[\"encoder_optimizer\"]\n",
    "    decoder_optimizer = checkpoint[\"decoder_optimizer\"]\n",
    "else:\n",
    "    epoch = 1\n",
    "#     encoder = EncoderRNN(input_size_enc, hidden_size).to(device)\n",
    "    # attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
    "#     decoder = DecoderRNN(input_size_dec, hidden_size, output_size).to(device)\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "if epoch <= epochs:\n",
    "    print(\"start training from epoch %d\" %epoch)\n",
    "    trainIters(train_data, encoder, decoder, encoder_optimizer, decoder_optimizer, epoch, epochs)\n",
    "else:\n",
    "    print(\"epoch trained (%d) exceeds maximum epochs (%d)\" %(epoch, epochs))\n",
    "\n",
    "print(\"finish training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    set_trace()\n",
    "    return pred\n",
    "\n",
    "def evaluate(input_tensor, target_tensor, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    accu_all = []\n",
    "    \n",
    "    loss = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hn = encoder.initHidden()\n",
    "        cn = encoder.initHidden()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "    #     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (hn, cn) = encoder(\n",
    "                input_tensor[ei], hn, cn)\n",
    "\n",
    "\n",
    "        decoder_input = target_tensor[0, :]\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, (hn, cn) = decoder(decoder_input, hn, cn) \n",
    "            loss += criterion(decoder_output, target_tensor[di, :].view(-1,).long())\n",
    "\n",
    "            decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            topv, topi = decoder_output.topk(1, dim=2)\n",
    "            \n",
    "            accu_all.append(accuracy(topi, target_tensor[di, :]))\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-28-6c65b19567e6>(3)accuracy()\n",
      "-> return pred\n",
      "(Pdb) pred.shape\n",
      "torch.Size([6, 35, 1])\n",
      "(Pdb) target.shape\n",
      "torch.Size([6, 35])\n",
      "(Pdb) pred\n",
      "tensor([[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]], device='cuda:0')\n",
      "(Pdb) target\n",
      "tensor([[37.0000, 39.0000, 50.0000, 35.0000, 47.0000, 39.0000, 38.0000, 36.0000,\n",
      "         41.0000, 52.0000, 44.0000, 49.0000, 44.0000, 34.0000, 43.0000, 33.0000,\n",
      "         44.0000, 22.0000, 35.8824, 34.0000, 39.0000, 17.0000, 41.0000, 15.0000,\n",
      "         11.0000, 10.0000, 28.0000, 25.0000, 27.0000, 34.0000, 42.0000, 41.0000,\n",
      "         38.0000, 51.0000, 40.0000],\n",
      "        [32.0000, 31.0000, 36.0000, 35.0000, 42.0000, 37.0000, 36.0000, 27.0000,\n",
      "         36.0000, 34.0000, 34.0000, 42.0000, 36.0000, 29.0000, 30.0000, 39.0000,\n",
      "         38.0000, 41.0000, 33.9062, 33.9062, 52.0000, 51.0000, 32.0000, 20.0000,\n",
      "         18.0000, 30.0000, 25.0000, 25.0000, 24.0000, 39.0000, 33.0000, 33.0000,\n",
      "         35.0000, 33.9062, 33.0000],\n",
      "        [26.0000, 26.0000, 31.0000, 30.9062, 31.0000, 28.0000, 28.0000, 42.0000,\n",
      "         37.0000, 29.0000, 30.0000, 43.0000, 34.0000, 27.0000, 30.0000, 29.0000,\n",
      "         32.0000, 37.0000, 30.9062, 39.0000, 46.0000, 35.0000, 17.0000, 34.0000,\n",
      "         14.0000, 30.9062, 26.0000, 31.0000, 26.0000, 40.0000, 30.0000, 28.0000,\n",
      "         25.0000, 34.0000, 24.0000],\n",
      "        [27.0000, 27.0000, 35.0000, 31.0000, 31.0000, 31.0000, 33.0000, 35.0000,\n",
      "         34.0000, 28.0000, 32.0000, 37.0000, 32.0000, 30.0000, 30.0000, 27.0000,\n",
      "         30.0000, 34.0000, 30.8182, 30.0000, 30.8182, 42.0000, 19.0000, 40.0000,\n",
      "         16.0000, 34.0000, 27.0000, 28.0000, 28.0000, 40.0000, 28.0000, 32.0000,\n",
      "         31.0000, 30.0000, 28.0000],\n",
      "        [32.0000, 33.0000, 33.0000, 34.0000, 35.0000, 31.0000, 39.0000, 44.0000,\n",
      "         31.0000, 42.0000, 40.0000, 48.0000, 38.0000, 29.0000, 29.0000, 26.0000,\n",
      "         25.0000, 35.0000, 32.9706, 29.0000, 40.0000, 35.0000, 21.0000, 36.0000,\n",
      "         11.0000, 37.0000, 26.0000, 33.0000, 27.0000, 38.0000, 29.0000, 35.0000,\n",
      "         31.0000, 37.0000, 32.0000],\n",
      "        [31.0000, 37.0000, 43.0000, 31.0000, 42.0000, 38.0000, 37.0000, 44.0000,\n",
      "         38.0000, 45.0000, 39.0000, 43.0000, 33.0000, 33.0000, 33.0000, 30.0000,\n",
      "         33.0000, 30.0000, 34.2121, 32.0000, 39.0000, 44.0000, 17.0000, 31.0000,\n",
      "          7.0000, 41.0000, 29.0000, 22.0000, 33.0000, 37.0000, 37.0000, 30.0000,\n",
      "         36.0000, 34.2121, 34.0000]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.from_numpy(pm2_5s_1_years[0][:,:24,:]).to(device)\n",
    "target_tensor = torch.from_numpy(pm2_5s_1_years[0][:,24:24+6,:]).to(device)\n",
    "evaluate(input_tensor, target_tensor, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2_5s_1_years[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
