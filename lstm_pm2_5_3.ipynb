{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L77Rq4wedYYz"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# if os.path.exists(\"../air_quality\"):\n",
    "#    !rm -rf \"../air_quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaqvGroEatoZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "\n",
    "from pdb import set_trace\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "SOURCE_URL = \"https://cloud.tsinghua.edu.cn/d/f519a587a6d943fa9aa0/files/?p=%2F%E5%8C%97%E4%BA%AC%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F.zip&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cU7gERKaatoh"
   },
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5e8dIEexatoi",
    "outputId": "1aa3662a-c039-47c2-dc18-ba0de319ee62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 北京空气质量.zip\n",
      "Extracting ../air_quality\\北京空气质量\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, work_directory):\n",
    "    \"\"\"Download the data from website, unless it's already here.\"\"\"\n",
    "    if not tf.io.gfile.exists(work_directory):\n",
    "        tf.io.gfile.makedirs(work_directory)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    # set_trace()\n",
    "    if not tf.io.gfile.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath)\n",
    "    with tf.io.gfile.GFile(filepath) as f:\n",
    "        print('Successfully downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "def extract_files(filepath):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    # filepath = Path(filepath)\n",
    "    print('Extracting', filepath)\n",
    "    #if not tf.io.gfile.exists(filepath):\n",
    "    #    tf.io.gfile.makedirs(filepath)\n",
    "        \n",
    "    with zipfile.ZipFile(filepath+\".zip\", 'r') as zip_ref:\n",
    "        #zip_ref.extractall(\"../air_quality/\")\n",
    "        \n",
    "        for member in zip_ref.infolist():\n",
    "            # set_trace()\n",
    "            \n",
    "            member.filename = member.filename.encode(\"cp437\").decode(\"utf8\")\n",
    "\n",
    "            zip_ref.extract(member, \"../air_quality/\")\n",
    "#             set_trace()\n",
    "# #             if member.filename == \"北京空气质量\"：\n",
    "# #                 os.rename(member.filename, \"pm_2_5_data\")\n",
    "#             set_trace()\n",
    "    #os.chdir(filepath) # change directory from working dir to dir with files\n",
    "\n",
    "    for item in os.listdir(filepath): # loop through items in dir\n",
    "        #set_trace()\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            #file_name = os.path.abspath(item) # get full path of files\n",
    "            zipfile_path = os.path.join(filepath, item)\n",
    "            zip_ref = zipfile.ZipFile(zipfile_path) # create zipfile object\n",
    "            zip_ref.extractall(filepath) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name) # delete zipped file\n",
    "\n",
    "def download_and_extract():\n",
    "    dest = \"../air_quality\"\n",
    "    file_name = \"北京空气质量.zip\"\n",
    "    filepath = maybe_download(file_name, dest)\n",
    "    filepath = filepath[:-4]\n",
    "    extract_files(filepath)\n",
    "download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEqoqRihatom"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data_pm2_5(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "def process_data(data, pm2_5=0):\n",
    "#     data = list(map(int, data))\n",
    "    N, d = data.shape\n",
    "    \n",
    "    new_data = np.zeros((N, d))\n",
    "#     new_data = []\n",
    "   \n",
    "    m = []\n",
    "    for j, row in enumerate(data):\n",
    "        row = [float(i) if i != \"\" else 0 for i in row]\n",
    "        row = np.array(row)\n",
    "       \n",
    "        if np.any(row==0):\n",
    "#             set_trace()\n",
    "            index = np.where(row==0)\n",
    "#             set_trace()\n",
    "            if len(index[0]) == len(row):\n",
    "                m.append(j)\n",
    "                continue\n",
    "            #for ind in index[0]:\n",
    "                \n",
    "            mu = np.sum(row)/(len(row)-len(index[0]))\n",
    "            row[index[0]] = mu\n",
    "#             set_trace()\n",
    "            if mu <= 0:\n",
    "                set_trace()\n",
    "#         set_trace()\n",
    "        new_data[j, :] = row\n",
    "        \n",
    "        \n",
    "    if m:\n",
    "        for ind in m:    \n",
    "            new_data[ind] = np.sum(new_data, axis=0)/(len(new_data)-len(m))\n",
    "    \n",
    "    if pm2_5:\n",
    "        new_data = normalize_data_pm2_5(new_data)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "def check_data(row, item, year):\n",
    "    check = [True if i==\"\" else False for i in row[3:]]\n",
    "    check_true = True\n",
    "    if np.all(np.array(check)):\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "           \n",
    "            check_true = False\n",
    "    return check_true\n",
    "\n",
    "def add_removed(item, year):\n",
    "    split = item.split(\"_\")\n",
    "    split[-2] = \"extra\"\n",
    "    item_1 = \"_\"\n",
    "    item_1 = item_1.join(split)\n",
    "    REMOVED_CSV[year].add(item_1)\n",
    "    split[-2] = \"all\"\n",
    "    item_2 = \"_\"\n",
    "    item_2 = item_2.join(split)\n",
    "    REMOVED_CSV[year].add(item_2)\n",
    "    \n",
    "\n",
    "def restructure_data(data):\n",
    "    num_doc = len(data)\n",
    "\n",
    "    N, d = data[0].shape\n",
    "    \n",
    "    new_data = np.zeros((num_doc, N, d))\n",
    "    \n",
    "    for i, dat in enumerate(data):\n",
    "        new_data[i, :, :] = dat\n",
    "    return new_data\n",
    "\n",
    "def clean(folderpath, consistent=False):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        \n",
    "        split = item.split(\"_\")\n",
    "        if \"beijing\" not in item:\n",
    "            REMOVED_CSV[year].add(item)\n",
    "            continue\n",
    "#         set_trace()\n",
    "        if consistent:\n",
    "            if split[-2] == \"extra\":\n",
    "                split_1 = split\n",
    "                split_1[-2] = \"all\"\n",
    "                item_1 = \"_\"\n",
    "                item_1 = item_1.join(split_1)\n",
    "\n",
    "                if item_1 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "            else:\n",
    "                split_2 = split\n",
    "                split_2[-2] = \"extra\"\n",
    "                item_2 = \"_\"\n",
    "                item_2 = item_2.join(split_2)\n",
    "                if item_2 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "        \n",
    "        \n",
    "        if item in REMOVED_CSV[year]:\n",
    "            continue\n",
    "        \n",
    "        filepath_csv = folderpath + \"/\" + item\n",
    "#         print(filepath_csv)\n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv, encoding=\"utf8\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    #         set_trace()\n",
    "            prev_header = []\n",
    "            for i, row in enumerate(csv_reader):\n",
    "#                 set_trace()\n",
    "                try:\n",
    "                    row[2]==\"SO2\"\n",
    "                        \n",
    "                except:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "                \n",
    "                if len(row[3:]) != 35:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "\n",
    "               \n",
    "                    \n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    if prev_header:\n",
    "                        if header != prev_header:\n",
    "                            set_trace()\n",
    "                            break\n",
    "                    prev_header = header\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "#                             REMOVED_CSV[year].add(item)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "\n",
    "        split = filepath_csv.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            if len(pm2_5) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "        elif split[-2] == \"extra\":\n",
    "            if len(SO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(NO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(CO) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(O3) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "    print(\"complete read folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "BMU7gIg9atoq",
    "outputId": "c32ea7c9-6ef1-4849-cd2c-4cfd49d37530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of REMOVED_CSV in year 2014: 2\n",
      "length of REMOVED_CSV in year 2015: 0\n",
      "length of REMOVED_CSV in year 2016: 0\n",
      "length of REMOVED_CSV in year 2017: 0\n",
      "length of REMOVED_CSV in year 2018: 0\n",
      "length of REMOVED_CSV in year 2019: 0\n",
      "length of REMOVED_CSV in year 2020: 0\n",
      "\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "length of REMOVED_CSV in year 2014: 83\n",
      "length of REMOVED_CSV in year 2015: 29\n",
      "length of REMOVED_CSV in year 2016: 64\n",
      "length of REMOVED_CSV in year 2017: 240\n",
      "length of REMOVED_CSV in year 2018: 118\n",
      "length of REMOVED_CSV in year 2019: 116\n",
      "length of REMOVED_CSV in year 2020: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REMOVED_CSV = {}\n",
    "REMOVED_CSV[\"2014\"] = {\"beijing_all_20141231.csv\", \"beijing_extra_20141231.csv\"}\n",
    "REMOVED_CSV[\"2015\"] = set()\n",
    "REMOVED_CSV[\"2016\"] = set()\n",
    "REMOVED_CSV[\"2017\"] = set()\n",
    "REMOVED_CSV[\"2018\"] = set()\n",
    "REMOVED_CSV[\"2019\"] = set()\n",
    "REMOVED_CSV[\"2020\"] = set()\n",
    "\n",
    "folderpath = \"../air_quality/北京空气质量\"\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")  \n",
    "\n",
    "\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            clean(folderpath+\"/\"+item)\n",
    "\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")            \n",
    "\n",
    "# print(\"Start checking REMOVED_CSV pairs\")\n",
    "# for key, values in REMOVED_CSV.items():\n",
    "#     for value_a in values:\n",
    "#         if \"beijing\" in value_a:\n",
    "#             two = 1\n",
    "#             split = value_a.split(\"_\")\n",
    "#             for value_b in values:\n",
    "#                 if value_a != value_b:\n",
    "#                     if split[-1] in value_b:\n",
    "#                         set_trace()\n",
    "#                         two += 1\n",
    "#                         break                    \n",
    "#             if two != 2:\n",
    "#                 set_trace()\n",
    "# print(\"Complete check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "x-nRRfjLatov",
    "outputId": "fed58a8b-d05b-4b1c-c1d4-b5e4e6f15460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def retrieve_data_alls(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    # set_trace()\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "            split = item.split(\"_\")\n",
    "            if split[-2] == \"extra\":\n",
    "                extras.append(item)\n",
    "            else:\n",
    "                alls.append(item)\n",
    "    alls.sort()\n",
    "    extras.sort()\n",
    "    alls_pairs = []\n",
    "    # set_trace()\n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = alls[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "\n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            alls_pairs.append((alls[i], alls[i+1]))\n",
    "    # set_trace()\n",
    "    for (item_1, item_2) in alls_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        pm2_5 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "            \n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 0)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s\n",
    "\n",
    "def retrieve_data_extras(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    \n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "\n",
    "    for item in items:\n",
    "        split = item.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            split_1 = split\n",
    "            split_1[-2] = \"extra\"\n",
    "            item_1 = \"_\"\n",
    "            item_1 = item_1.join(split_1)\n",
    "            if item_1 in items:\n",
    "                extras.append(item_1)\n",
    "                alls.append(item)\n",
    "    \n",
    "    alls.sort()\n",
    "    extras.sort()\n",
    "    assert(len(extras)==len(alls))\n",
    "    extra_all_pairs = []\n",
    "    \n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = extras[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "        \n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            extra_all_pairs.append((extras[i], alls[i+1]))\n",
    "        \n",
    "#     set_trace()\n",
    "    for (item_1, item_2) in extra_all_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "           \n",
    "        \n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "                        \n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 0)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "        if SO2:\n",
    "            SO2 = np.array(SO2)\n",
    "#             set_trace()\n",
    "            SO2 = process_data(SO2)\n",
    "#             set_trace()\n",
    "            SO2s.append(SO2)\n",
    "\n",
    "        if NO2:\n",
    "            NO2 = np.array(NO2)\n",
    "            NO2 = process_data(NO2)\n",
    "            NO2s.append(NO2)\n",
    "\n",
    "        if CO:\n",
    "            CO = np.array(CO)\n",
    "            CO = process_data(CO)\n",
    "            COs.append(CO)\n",
    "\n",
    "        if O3:\n",
    "            O3 = np.array(O3)\n",
    "            O3 = process_data(O3)\n",
    "            O3s.append(O3)\n",
    "    \n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "    SO2s = restructure_data(SO2s)\n",
    "    NO2s = restructure_data(NO2s)\n",
    "    COs = restructure_data(COs)\n",
    "    O3s = restructure_data(O3s)\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s, SO2s, NO2s, COs, O3s\n",
    "#     return pm2_5s\n",
    "\n",
    "pm2_5s_1_years = []\n",
    "pm2_5s_2_years = []\n",
    "SO2s_years = []\n",
    "NO2s_years = []\n",
    "COs_years = []\n",
    "O3s_years = []\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            pm2_5s_1 = retrieve_data_alls(folderpath+\"/\"+item)\n",
    "            pm2_5s_1_years.append(pm2_5s_1)\n",
    "            \n",
    "            pm2_5s_2, SO2s, NO2s, COs, O3s = retrieve_data_extras(folderpath+\"/\"+item)  \n",
    "            pm2_5s_2_years.append(pm2_5s_2)\n",
    "            SO2s_years.append(SO2s)\n",
    "            NO2s_years.append(NO2s)\n",
    "            COs_years.append(COs)\n",
    "            O3s_years.append(O3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gf04_Ltkatoz",
    "outputId": "5def18f6-5c51-4b6f-9a20-269d79398e35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[46.        , 46.        , 48.        , ..., 30.        ,\n",
       "         19.        , 11.        ],\n",
       "        [41.        , 40.        , 44.        , ..., 27.        ,\n",
       "         36.79411765, 31.        ],\n",
       "        [29.        , 32.        , 35.        , ..., 25.        ,\n",
       "         32.09090909,  2.        ],\n",
       "        ...,\n",
       "        [50.        , 32.        , 47.        , ..., 18.        ,\n",
       "          2.        ,  3.        ],\n",
       "        [34.        , 20.        , 26.        , ...,  3.        ,\n",
       "         24.08823529,  2.        ],\n",
       "        [30.        , 33.        , 12.        , ...,  2.        ,\n",
       "         13.        , 10.        ]],\n",
       "\n",
       "       [[44.        , 52.        , 34.        , ..., 20.        ,\n",
       "         27.        , 31.        ],\n",
       "        [45.        , 48.        , 44.        , ..., 30.        ,\n",
       "         33.78787879, 40.        ],\n",
       "        [51.        , 57.        , 55.        , ..., 39.        ,\n",
       "         28.        , 47.        ],\n",
       "        ...,\n",
       "        [78.        , 69.        , 84.        , ..., 64.        ,\n",
       "         35.        , 30.        ],\n",
       "        [63.        , 71.        , 73.        , ..., 57.        ,\n",
       "         79.70588235, 29.        ],\n",
       "        [65.        , 66.        , 62.        , ..., 39.        ,\n",
       "         37.        , 35.        ]],\n",
       "\n",
       "       [[22.        , 11.        , 10.        , ..., 10.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.        ,  7.        ,  2.        , ...,  2.        ,\n",
       "         18.12121212,  2.        ],\n",
       "        [ 3.        ,  4.        ,  2.        , ...,  2.        ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [52.        , 63.        , 86.        , ..., 33.        ,\n",
       "          8.        ,  3.        ],\n",
       "        [ 3.        , 18.        , 67.        , ..., 22.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [11.        ,  5.        , 29.        , ...,  7.        ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.5       ,  8.        ,  2.        , ...,  3.7       ,\n",
       "          2.        ,  6.7       ],\n",
       "        [ 2.7       ,  7.8       ,  2.        , ...,  2.        ,\n",
       "          2.        ,  7.        ],\n",
       "        [ 5.1       ,  7.6       ,  2.        , ...,  2.7       ,\n",
       "          2.        ,  5.2       ],\n",
       "        ...,\n",
       "        [ 2.        ,  9.1       ,  2.        , ...,  5.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  8.5       ,  2.1       , ...,  4.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  9.        ,  2.7       , ...,  4.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[10.371875  , 12.1       ,  6.6       , ...,  3.3       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 9.26875   , 11.7       ,  6.        , ...,  2.9       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 8.628125  , 11.2       ,  5.8       , ...,  2.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 6.        , 12.8       ,  7.9       , ...,  6.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.8       ,  8.2       , ...,  5.8       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.5       ,  7.5       , ...,  5.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[ 6.5       , 12.5       ,  7.9       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 13.3       ,  7.        , ...,  6.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 13.4       ,  6.2       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 5.7       , 11.2       ,  6.9       , ...,  4.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.9       , 11.5       ,  7.        , ...,  3.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 12.4       ,  7.3       , ...,  2.3       ,\n",
       "          2.        ,  2.        ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pm2_5s_2_years[0]\n",
    "O3s_years[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wn44ZrQiato4"
   },
   "outputs": [],
   "source": [
    "def normalize_concat(arr_data):\n",
    "    norm_arr_data = []\n",
    "    for data in arr_data:\n",
    "        norm_arr_data.append(normalize(data))\n",
    "#     set_trace()\n",
    "    return concat(norm_arr_data)\n",
    "\n",
    "def normalize(data):\n",
    "    new_data = []\n",
    "    for dat in data:\n",
    "#         set_trace()\n",
    "        temp = (dat-np.min(dat))/(np.max(dat)-np.min(dat))\n",
    "        new_data.append(temp)\n",
    "    \n",
    "#     set_trace()\n",
    "    return new_data\n",
    "\n",
    "def concat(arr_data):\n",
    "    years = len(arr_data[0])\n",
    "    new_data = []\n",
    "    for year in range(years):\n",
    "        temp = []\n",
    "        for data in arr_data:\n",
    "            temp.append(data[year])\n",
    "        \n",
    "        new_data.append(np.concatenate(temp, 0))\n",
    "#         set_trace()\n",
    "    return new_data\n",
    "\n",
    "feature_data = normalize_concat((SO2s_years, NO2s_years, O3s_years, COs_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "-zwKSHjZato8",
    "outputId": "598ccecc-3bb4-415c-d268-b8479c10e71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years: 7\n",
      "data for 2015: (836, 24, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"years:\", len(feature_data))\n",
    "print(\"data for 2015:\", feature_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mN6QexKcatpA"
   },
   "outputs": [],
   "source": [
    "def concat_years(arr_data):\n",
    "    years = len(arr_data)\n",
    "    sum_doc = 0\n",
    "    N, h, f = arr_data[0].shape\n",
    "    for year in range(years):\n",
    "        sum_doc += arr_data[year].shape[0]\n",
    "    new_data = np.zeros((sum_doc, h, f))\n",
    "    ind = 0\n",
    "    for year in range(years):\n",
    "        ind_last = arr_data[year].shape[0]\n",
    "        new_data[ind:ind+ind_last, :, :] = arr_data[year]\n",
    "        ind = ind + ind_last\n",
    "    return new_data    \n",
    "# train_data = concat_years(pm2_5s_1_years[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NleZ4Q5atpE"
   },
   "source": [
    "Training Model: seq2seq <br>\n",
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6tnvW5ratpF"
   },
   "outputs": [],
   "source": [
    "\n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "# #         embedded = self.embedding(input).view(1, 1, -1)\n",
    "# #         output = embedded\n",
    "#         output = input.view(24,1,6)\n",
    "# #         set_trace()\n",
    "#         output, hidden = self.lstm(output.float(), (hn.detach(), cn.detach()))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "# #         h_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         c_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         if torch.cuda.is_available():\n",
    "# #             return h_hidden.cuda(), c_hidden.cuda()\n",
    "# #         else:\n",
    "# #             return h_hidden, c_hidden\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-l_gd9_LatpI"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwbjJRVNatpJ"
   },
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "#         output = input.view(24, self.hidden_size)\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.lstm(output, (hn, cn))\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "        \n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AVBDlT4catpO",
    "outputId": "499acc07-1ff6-424e-e75a-98d5e99ddafe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 37.        ,  39.        ,  50.        , ...,  38.        ,\n",
       "           51.        ,  40.        ],\n",
       "         [ 32.        ,  31.        ,  36.        , ...,  35.        ,\n",
       "           33.90625   ,  33.        ],\n",
       "         [ 26.        ,  26.        ,  31.        , ...,  25.        ,\n",
       "           34.        ,  24.        ],\n",
       "         ...,\n",
       "         [ 15.        ,  22.        ,  17.        , ...,  19.        ,\n",
       "           20.        ,  23.        ],\n",
       "         [ 30.        ,  26.        ,  12.        , ...,  17.        ,\n",
       "           20.35294118,  22.        ],\n",
       "         [ 37.        ,  31.        ,  24.        , ...,  26.        ,\n",
       "           34.        ,  31.        ]],\n",
       " \n",
       "        [[ 29.        ,  27.        ,  32.        , ...,  31.        ,\n",
       "           37.        ,  31.        ],\n",
       "         [ 27.        ,  29.        ,  32.        , ...,  34.        ,\n",
       "           39.        ,  28.        ],\n",
       "         [ 34.        ,  29.        ,  35.        , ...,  33.        ,\n",
       "           34.        ,  33.        ],\n",
       "         ...,\n",
       "         [ 48.        ,  52.        ,  51.        , ...,  50.        ,\n",
       "           44.55882353,  45.        ],\n",
       "         [ 56.        ,  53.        ,  49.        , ...,  47.        ,\n",
       "           67.        ,  41.        ],\n",
       "         [ 55.        ,  46.        ,  56.        , ...,  54.        ,\n",
       "           62.        ,  46.        ]],\n",
       " \n",
       "        [[ 88.        ,  74.        ,  89.        , ...,  89.        ,\n",
       "           82.32352941,  83.        ],\n",
       "         [ 92.        ,  78.        ,  88.        , ...,  78.        ,\n",
       "          102.        ,  76.        ],\n",
       "         [ 92.        ,  79.        ,  93.        , ...,  84.        ,\n",
       "           85.29411765,  71.        ],\n",
       "         ...,\n",
       "         [120.        , 107.        , 109.        , ..., 120.        ,\n",
       "          131.        , 108.        ],\n",
       "         [ 90.        ,  98.        , 110.        , ..., 114.        ,\n",
       "          105.26470588,  84.        ],\n",
       "         [ 94.        ,  92.        , 113.        , ..., 106.        ,\n",
       "          107.96969697,  99.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 64.6       ,  65.1       , 102.6       , ..., 106.5       ,\n",
       "          127.2       , 113.7       ],\n",
       "         [ 94.2       ,  79.8       ,  80.1       , ...,  88.5       ,\n",
       "          147.9       , 113.7       ],\n",
       "         [121.9       ,  97.7       ,  92.        , ..., 133.8       ,\n",
       "          153.1       , 115.4       ],\n",
       "         ...,\n",
       "         [247.        , 206.7       , 176.1       , ..., 222.8       ,\n",
       "          261.5       , 283.6       ],\n",
       "         [230.8       , 211.3       , 190.2       , ..., 219.6       ,\n",
       "          262.        , 260.7       ],\n",
       "         [188.4       , 211.3       , 170.4       , ..., 193.1       ,\n",
       "          230.9       , 249.4       ]],\n",
       " \n",
       "        [[252.9       , 302.6       , 258.1       , ..., 320.7       ,\n",
       "          386.2       , 365.4       ],\n",
       "         [273.6       , 316.9       , 244.9       , ..., 319.6       ,\n",
       "          395.6       , 391.        ],\n",
       "         [263.5       , 321.        , 210.        , ..., 289.        ,\n",
       "          385.5       , 398.9       ],\n",
       "         ...,\n",
       "         [ 85.9       ,  92.3       , 108.2       , ..., 123.8       ,\n",
       "          186.7       , 136.5       ],\n",
       "         [ 85.1       , 112.8       , 116.8       , ..., 145.7       ,\n",
       "          224.4       , 152.9       ],\n",
       "         [ 87.4       , 143.9       , 105.5       , ..., 137.9       ,\n",
       "          224.2       , 189.3       ]],\n",
       " \n",
       "        [[106.2       , 143.3       ,  85.        , ..., 117.6       ,\n",
       "          214.        , 189.6       ],\n",
       "         [ 53.7       , 154.8       ,  38.9       , ...,  49.1       ,\n",
       "          174.4       , 138.1       ],\n",
       "         [ 44.9       , 128.6       ,  18.9       , ...,  21.4       ,\n",
       "          101.6       , 108.2       ],\n",
       "         ...,\n",
       "         [ 13.6       ,  19.4       ,  15.5       , ...,  23.6       ,\n",
       "           22.8       ,  17.7       ],\n",
       "         [  3.2       ,  11.9       ,  10.        , ...,  10.4       ,\n",
       "           14.6       ,   3.        ],\n",
       "         [  3.        ,  11.4       ,  17.4       , ...,  12.8       ,\n",
       "           19.3       ,   3.        ]]]),\n",
       " array([[[159.4, 150.4, 124.4, ..., 148. , 193.3, 150.8],\n",
       "         [170.5, 158.2, 145. , ..., 162. , 210.4, 171.3],\n",
       "         [185.2, 163.6, 137.5, ..., 144.5, 220.7, 179.9],\n",
       "         ...,\n",
       "         [286. , 231.4, 207.5, ..., 214.3, 296.3, 303.4],\n",
       "         [294.2, 248.7, 212.8, ..., 255.4, 297.5, 306.8],\n",
       "         [304.1, 241.2, 208.6, ..., 259.7, 294.4, 347.6]],\n",
       " \n",
       "        [[276.8, 229.2, 193.5, ..., 248.1, 308.4, 389.7],\n",
       "         [269.9, 224.1, 193.1, ..., 265.7, 288.1, 439.5],\n",
       "         [227.3, 223.8, 192.6, ..., 247.2, 283.3, 375.6],\n",
       "         ...,\n",
       "         [345.4, 270. , 236.6, ..., 303.1, 318.3, 342.8],\n",
       "         [332.8, 292.2, 252.2, ..., 327.5, 365.8, 304.4],\n",
       "         [251.8, 198.1, 226.7, ..., 275.6, 262. , 338.6]],\n",
       " \n",
       "        [[245.7, 179.2, 170.2, ..., 214. , 210.2, 332.5],\n",
       "         [188.4, 147.6, 142.7, ..., 188.5, 178.9, 255.1],\n",
       "         [144.9, 132.1, 108.6, ..., 137.6, 156.4, 228.1],\n",
       "         ...,\n",
       "         [  6.1,  10.5,   4.3, ...,   9.5,   5.4,   8.4],\n",
       "         [  3.5,   5.1,   4. , ...,   3.9,   6.8,   4.5],\n",
       "         [  5.9,   5.3,   5.6, ...,   9. ,   7.3,   4.1]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[281. , 259. , 223. , ..., 252. , 322. , 277. ],\n",
       "         [257. , 276. , 225. , ..., 267. , 343. , 312. ],\n",
       "         [260. , 282. , 193. , ..., 213. , 343. , 311. ],\n",
       "         ...,\n",
       "         [554. , 488. , 506. , ..., 541. , 534. , 584. ],\n",
       "         [555. , 497. , 519. , ..., 560. , 565. , 524. ],\n",
       "         [513. , 496. , 504. , ..., 381. , 550. , 491. ]],\n",
       " \n",
       "        [[475. , 479. , 362. , ..., 355. , 506. , 467. ],\n",
       "         [436. , 471. , 348. , ..., 377. , 471. , 516. ],\n",
       "         [273. , 398. , 277. , ..., 386. , 426. , 551. ],\n",
       "         ...,\n",
       "         [ 27. ,  24. ,  25. , ...,  25. ,  23. ,  19. ],\n",
       "         [ 20. ,  13. ,  15. , ...,  17. ,  18. ,  34. ],\n",
       "         [ 18. ,  18. ,  14. , ...,  17. ,  17. ,  35. ]],\n",
       " \n",
       "        [[ 37. ,  18. ,  18. , ...,  29. ,  20. ,  32. ],\n",
       "         [ 21. ,  16. ,  28. , ...,  33. ,  27. ,  33. ],\n",
       "         [ 25. ,  26. ,  26. , ...,  30. ,  34. ,  34. ],\n",
       "         ...,\n",
       "         [157. , 141. , 122. , ..., 134. , 153. , 199. ],\n",
       "         [171. , 181. , 134. , ..., 125. , 159. , 231. ],\n",
       "         [204. , 203. , 145. , ..., 187. , 234. , 242. ]]]),\n",
       " array([[[314.        , 286.        , 289.        , ..., 328.        ,\n",
       "          306.        , 449.        ],\n",
       "         [345.        , 311.        , 305.        , ..., 328.        ,\n",
       "          309.        , 434.        ],\n",
       "         [306.        , 322.        , 287.        , ..., 300.        ,\n",
       "          348.        , 428.        ],\n",
       "         ...,\n",
       "         [499.        , 377.        , 377.        , ..., 432.        ,\n",
       "          506.        , 509.        ],\n",
       "         [505.        , 479.        , 498.        , ..., 496.        ,\n",
       "          538.        , 531.        ],\n",
       "         [504.        , 482.        , 485.        , ..., 525.        ,\n",
       "          534.        , 559.        ]],\n",
       " \n",
       "        [[535.        , 512.        , 519.        , ..., 534.        ,\n",
       "          574.        , 430.61764706],\n",
       "         [520.        , 519.        , 526.        , ..., 556.        ,\n",
       "          556.        , 620.        ],\n",
       "         [482.        , 500.        , 489.        , ..., 528.        ,\n",
       "          490.        , 622.        ],\n",
       "         ...,\n",
       "         [227.        , 199.        , 220.        , ..., 239.        ,\n",
       "          254.        , 223.        ],\n",
       "         [254.        , 198.        , 231.        , ..., 246.        ,\n",
       "          272.        , 253.        ],\n",
       "         [247.        , 209.        , 238.        , ..., 273.        ,\n",
       "          280.        , 254.        ]],\n",
       " \n",
       "        [[242.        , 216.        , 245.        , ..., 289.        ,\n",
       "          276.        , 242.        ],\n",
       "         [252.        , 227.        , 219.        , ..., 297.        ,\n",
       "          285.        , 237.        ],\n",
       "         [251.        , 240.        ,  78.        , ..., 260.        ,\n",
       "          290.        , 270.        ],\n",
       "         ...,\n",
       "         [ 13.        ,  17.        ,  11.        , ...,  17.        ,\n",
       "           14.        ,  13.        ],\n",
       "         [ 15.        ,  11.        ,  10.        , ...,  14.        ,\n",
       "           14.        ,   9.        ],\n",
       "         [ 14.        ,  18.        ,  10.        , ...,  14.        ,\n",
       "           13.        ,  20.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[445.        , 378.        , 397.        , ..., 398.        ,\n",
       "          450.        , 451.        ],\n",
       "         [434.        , 368.        , 386.        , ..., 408.        ,\n",
       "          452.        , 436.        ],\n",
       "         [424.        , 354.        , 286.        , ..., 375.        ,\n",
       "          444.        , 455.        ],\n",
       "         ...,\n",
       "         [ 12.        ,  11.        ,  15.        , ...,  13.        ,\n",
       "           13.        ,  12.        ],\n",
       "         [ 12.        ,   6.        ,   9.        , ...,  15.        ,\n",
       "           12.        ,  17.        ],\n",
       "         [ 13.        ,   8.        ,  15.        , ...,  15.        ,\n",
       "           11.        ,  23.        ]],\n",
       " \n",
       "        [[ 12.        ,  11.        ,  11.        , ...,  15.        ,\n",
       "           15.        ,  18.        ],\n",
       "         [ 10.        ,  10.        ,   9.        , ...,  22.        ,\n",
       "           10.        ,  18.        ],\n",
       "         [  9.        ,  16.        ,  11.        , ...,  18.        ,\n",
       "           14.        ,  18.        ],\n",
       "         ...,\n",
       "         [121.        , 115.        , 113.        , ..., 113.        ,\n",
       "          121.        , 114.        ],\n",
       "         [124.        , 131.        , 122.        , ..., 122.        ,\n",
       "          137.        , 110.        ],\n",
       "         [148.        , 148.        , 130.        , ..., 141.        ,\n",
       "          156.        , 133.        ]],\n",
       " \n",
       "        [[153.        , 146.        , 154.        , ..., 161.        ,\n",
       "          170.        , 136.        ],\n",
       "         [158.        , 151.        , 152.        , ..., 162.        ,\n",
       "          173.        , 144.        ],\n",
       "         [167.        , 166.        , 158.        , ..., 167.        ,\n",
       "          189.        , 153.        ],\n",
       "         ...,\n",
       "         [ 13.        ,   8.        ,   8.        , ...,  16.        ,\n",
       "           14.        ,  13.        ],\n",
       "         [ 11.        ,  11.        ,   6.        , ...,   7.        ,\n",
       "           12.        ,   8.        ],\n",
       "         [  7.        ,   7.        ,  15.        , ...,  10.        ,\n",
       "            9.        ,   7.        ]]]),\n",
       " array([[[262.        , 250.        , 212.        , ..., 227.        ,\n",
       "          404.        , 297.        ],\n",
       "         [280.        , 288.        , 233.        , ..., 260.        ,\n",
       "          426.        , 291.02941176],\n",
       "         [270.        , 324.        , 277.        , ..., 293.        ,\n",
       "          440.        , 344.        ],\n",
       "         ...,\n",
       "         [572.        , 481.        , 461.54545455, ..., 461.54545455,\n",
       "          633.        , 499.        ],\n",
       "         [584.        , 494.        , 456.66666667, ..., 560.        ,\n",
       "          631.        , 500.        ],\n",
       "         [599.        , 496.        , 442.09677419, ..., 557.        ,\n",
       "          627.        , 547.        ]],\n",
       " \n",
       "        [[615.        , 524.        , 437.32352941, ..., 572.        ,\n",
       "          627.        , 562.        ],\n",
       "         [662.        , 522.        , 440.84848485, ..., 619.        ,\n",
       "          652.        , 440.84848485],\n",
       "         [654.        , 534.        , 426.46875   , ..., 650.        ,\n",
       "          618.        , 513.        ],\n",
       "         ...,\n",
       "         [395.        , 357.        , 378.        , ..., 394.        ,\n",
       "          372.        , 373.        ],\n",
       "         [405.        , 349.        , 382.        , ..., 395.        ,\n",
       "          379.        , 357.        ],\n",
       "         [351.        , 301.        , 370.        , ..., 376.        ,\n",
       "          322.        , 241.        ]],\n",
       " \n",
       "        [[234.        , 223.        , 267.        , ..., 258.        ,\n",
       "          265.        , 160.        ],\n",
       "         [175.        , 155.        , 189.        , ..., 190.        ,\n",
       "          175.        , 145.        ],\n",
       "         [167.        , 153.        , 173.        , ..., 168.        ,\n",
       "          147.        , 143.        ],\n",
       "         ...,\n",
       "         [221.        , 174.        , 210.        , ..., 210.        ,\n",
       "          211.        , 180.        ],\n",
       "         [219.        , 173.        , 201.        , ..., 200.        ,\n",
       "          233.        , 191.        ],\n",
       "         [199.        , 177.        , 196.        , ..., 217.        ,\n",
       "          217.        , 212.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[157.        , 144.        , 158.        , ..., 168.        ,\n",
       "          205.        , 168.        ],\n",
       "         [166.        , 147.        , 170.        , ..., 178.        ,\n",
       "          212.        , 178.        ],\n",
       "         [175.        , 149.        , 171.        , ..., 185.        ,\n",
       "          213.        , 194.        ],\n",
       "         ...,\n",
       "         [213.        , 193.        , 202.        , ..., 222.        ,\n",
       "          225.        , 230.        ],\n",
       "         [210.        , 191.        , 201.        , ..., 217.        ,\n",
       "          229.        , 235.        ],\n",
       "         [212.        , 195.        , 198.        , ..., 215.        ,\n",
       "          228.        , 229.        ]],\n",
       " \n",
       "        [[211.        , 197.        , 198.        , ..., 213.        ,\n",
       "          224.        , 222.        ],\n",
       "         [209.        , 195.        , 200.        , ..., 216.        ,\n",
       "          222.        , 224.        ],\n",
       "         [209.        , 188.        , 195.        , ..., 212.        ,\n",
       "          219.        , 225.        ],\n",
       "         ...,\n",
       "         [ 24.        ,  14.        ,  13.        , ...,  21.        ,\n",
       "            6.        ,  11.        ],\n",
       "         [ 20.        ,  17.        ,  16.        , ...,  18.        ,\n",
       "            7.        ,  10.        ],\n",
       "         [ 18.        ,  13.        ,  12.        , ...,  16.        ,\n",
       "            6.        ,   8.        ]],\n",
       " \n",
       "        [[ 16.        ,   6.        ,   8.        , ...,   8.        ,\n",
       "            6.        ,   6.        ],\n",
       "         [ 10.        ,   8.        ,   7.        , ...,  10.        ,\n",
       "            4.        ,   5.        ],\n",
       "         [  8.        ,   3.        ,  12.        , ...,   9.        ,\n",
       "            4.        ,   4.        ],\n",
       "         ...,\n",
       "         [ 62.        ,  57.        ,  59.        , ...,  71.        ,\n",
       "           66.        ,  54.        ],\n",
       "         [ 69.        ,  58.        ,  53.        , ...,  62.        ,\n",
       "           69.        ,  61.        ],\n",
       "         [ 72.        ,  60.        ,  53.        , ...,  70.        ,\n",
       "           56.        ,  75.        ]]]),\n",
       " array([[[32.   , 32.   , 28.   , ..., 36.   , 30.   , 37.   ],\n",
       "         [36.   , 31.   , 26.   , ..., 36.   , 28.   , 35.   ],\n",
       "         [36.   , 30.   , 25.   , ..., 36.   , 36.   , 38.   ],\n",
       "         ...,\n",
       "         [ 9.   ,  8.   ,  7.   , ...,  5.   ,  7.   ,  6.   ],\n",
       "         [ 5.   ,  5.   ,  8.   , ...,  9.   ,  7.   ,  6.   ],\n",
       "         [ 9.   ,  6.   ,  7.   , ...,  8.   , 11.625, 11.625]],\n",
       " \n",
       "        [[ 6.   ,  3.   ,  5.   , ...,  7.   ,  6.   ,  5.   ],\n",
       "         [ 6.   ,  3.   ,  7.   , ..., 13.   ,  6.   ,  4.   ],\n",
       "         [ 7.   ,  3.   , 10.   , ..., 12.   ,  7.   ,  6.   ],\n",
       "         ...,\n",
       "         [17.   , 16.   , 16.   , ..., 20.   , 17.   , 15.   ],\n",
       "         [13.   , 18.   , 20.   , ..., 19.   , 20.   , 18.   ],\n",
       "         [14.   , 13.   , 17.   , ..., 18.   , 19.   , 11.   ]],\n",
       " \n",
       "        [[14.   , 14.   ,  9.   , ..., 18.   , 17.   , 11.   ],\n",
       "         [11.   ,  4.   , 12.   , ..., 13.   , 12.   , 12.   ],\n",
       "         [ 6.   ,  9.   , 10.   , ..., 13.   , 10.   , 13.   ],\n",
       "         ...,\n",
       "         [23.   , 18.   , 23.   , ..., 31.   , 28.   , 19.   ],\n",
       "         [26.   , 27.   , 22.   , ..., 29.   , 34.   , 22.   ],\n",
       "         [29.   , 23.   , 27.   , ..., 32.   , 41.   , 21.   ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8.   ,  8.   , 11.   , ...,  8.   , 37.   , 10.   ],\n",
       "         [ 6.   , 15.   ,  6.   , ...,  8.   , 40.   ,  8.   ],\n",
       "         [ 8.   ,  6.   ,  5.   , ...,  6.   , 35.   ,  8.   ],\n",
       "         ...,\n",
       "         [15.   , 11.   ,  9.   , ..., 14.   , 14.   , 23.   ],\n",
       "         [ 7.   , 11.   , 15.   , ..., 14.   , 12.   , 21.   ],\n",
       "         [ 5.   ,  5.   , 10.   , ..., 14.   ,  9.   , 20.   ]],\n",
       " \n",
       "        [[ 7.   , 10.   ,  5.   , ...,  9.   , 10.   , 19.   ],\n",
       "         [ 8.   ,  7.   ,  6.   , ...,  7.   , 12.   , 13.   ],\n",
       "         [ 9.   ,  7.   ,  8.   , ..., 12.   ,  9.   , 12.   ],\n",
       "         ...,\n",
       "         [37.   , 22.   , 30.   , ..., 31.   , 39.   , 27.   ],\n",
       "         [34.   , 22.   , 26.   , ..., 29.   , 40.   , 33.   ],\n",
       "         [28.   , 24.   , 29.   , ..., 32.   , 41.   , 43.   ]],\n",
       " \n",
       "        [[31.   , 24.   , 34.   , ..., 38.   , 43.   , 45.   ],\n",
       "         [34.   , 31.   , 42.   , ..., 48.   , 49.   , 43.   ],\n",
       "         [34.   , 32.   , 42.   , ..., 38.   , 46.   , 53.   ],\n",
       "         ...,\n",
       "         [54.   , 57.   , 52.   , ..., 62.   , 72.   , 52.   ],\n",
       "         [54.   , 52.   , 57.   , ..., 57.   , 72.   , 49.   ],\n",
       "         [58.   , 55.   , 50.   , ..., 53.   , 60.   , 51.   ]]]),\n",
       " array([[[ 16.        ,  17.        ,  18.        , ...,  22.        ,\n",
       "           18.        ,  14.        ],\n",
       "         [ 18.        ,  19.        ,  16.        , ...,  25.        ,\n",
       "           17.        ,  32.57575758],\n",
       "         [ 22.        ,  17.        ,  22.        , ...,  20.        ,\n",
       "           18.        ,  15.        ],\n",
       "         ...,\n",
       "         [181.        , 151.        , 178.        , ..., 175.        ,\n",
       "          239.        , 228.        ],\n",
       "         [211.        , 195.        , 174.        , ..., 179.        ,\n",
       "          230.        , 241.        ],\n",
       "         [214.        , 211.        , 186.        , ..., 180.        ,\n",
       "          223.        , 254.        ]],\n",
       " \n",
       "        [[217.        , 218.        , 193.        , ..., 190.        ,\n",
       "          239.        , 256.        ],\n",
       "         [231.        , 229.        , 235.        , ..., 218.        ,\n",
       "          260.        , 235.        ],\n",
       "         [232.        , 146.4516129 , 258.        , ..., 244.        ,\n",
       "          264.        , 221.        ],\n",
       "         ...,\n",
       "         [126.        , 120.        , 148.        , ..., 167.        ,\n",
       "          104.        , 128.        ],\n",
       "         [132.        , 116.        , 181.        , ..., 162.        ,\n",
       "          112.        , 133.        ],\n",
       "         [143.        , 111.        , 164.        , ..., 158.        ,\n",
       "          115.        , 135.        ]],\n",
       " \n",
       "        [[131.        , 120.        , 150.        , ..., 146.        ,\n",
       "          133.        , 138.        ],\n",
       "         [ 77.        ,  96.        ,  66.        , ...,  59.        ,\n",
       "          102.        , 100.        ],\n",
       "         [ 30.        ,  29.        ,  22.        , ...,  30.        ,\n",
       "           40.        ,  25.        ],\n",
       "         ...,\n",
       "         [ 15.        ,  17.        ,  14.        , ...,  17.        ,\n",
       "           12.        ,   9.        ],\n",
       "         [ 11.        ,  14.        ,  14.        , ...,  16.        ,\n",
       "           13.        ,   7.        ],\n",
       "         [  8.        ,   9.        ,   6.        , ...,  14.        ,\n",
       "           13.        ,   7.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[106.        , 131.        ,  94.        , ..., 108.        ,\n",
       "          119.        , 115.        ],\n",
       "         [119.        , 121.        , 100.        , ..., 111.        ,\n",
       "          115.        , 122.        ],\n",
       "         [122.        , 115.        , 103.        , ..., 106.        ,\n",
       "          119.        , 129.        ],\n",
       "         ...,\n",
       "         [  5.        ,   5.        ,   8.        , ...,  12.        ,\n",
       "            6.        ,   2.        ],\n",
       "         [  7.        ,   9.        ,   6.        , ...,  11.        ,\n",
       "            6.        ,   2.        ],\n",
       "         [ 10.        ,   6.        ,   8.        , ...,   7.        ,\n",
       "            6.        ,   2.        ]],\n",
       " \n",
       "        [[  5.        ,   8.        ,   8.        , ...,  12.        ,\n",
       "            7.        ,   4.        ],\n",
       "         [  7.        ,  14.        ,   8.        , ...,  15.        ,\n",
       "            7.        ,   4.        ],\n",
       "         [ 12.        ,  12.        ,   5.        , ...,  13.        ,\n",
       "            7.        ,   4.        ],\n",
       "         ...,\n",
       "         [  9.        ,   8.        ,   8.        , ...,  21.        ,\n",
       "            9.        ,   5.        ],\n",
       "         [  7.        ,   6.        ,   4.        , ...,  12.        ,\n",
       "            8.        ,   6.        ],\n",
       "         [ 11.        ,   5.        ,   7.        , ...,  11.        ,\n",
       "            8.        ,   5.        ]],\n",
       " \n",
       "        [[  8.        ,   7.        ,  10.        , ...,  14.        ,\n",
       "            9.        ,   6.        ],\n",
       "         [  6.        ,  13.        ,   4.        , ...,   9.        ,\n",
       "            9.        ,   5.        ],\n",
       "         [  7.        ,   2.        ,   5.        , ...,   9.        ,\n",
       "           10.        ,   6.        ],\n",
       "         ...,\n",
       "         [ 44.        ,  39.        ,  38.        , ...,  52.        ,\n",
       "           45.        ,  41.        ],\n",
       "         [ 46.        ,  45.        ,  35.        , ...,  46.        ,\n",
       "           47.        ,  42.        ],\n",
       "         [ 38.        ,  46.        ,  34.        , ...,  41.        ,\n",
       "           46.        ,  45.        ]]]),\n",
       " array([[[ 48.,  43.,  53., ...,  57.,  47.,  73.],\n",
       "         [ 56.,  46.,  52., ...,  59.,  47.,  80.],\n",
       "         [ 60.,  57.,  54., ...,  61.,  60.,  77.],\n",
       "         ...,\n",
       "         [ 71.,  70.,  71., ...,  77.,  75.,  54.],\n",
       "         [ 73.,  76.,  73., ...,  77.,  82.,  61.],\n",
       "         [ 75.,  72.,  69., ...,  70.,  84.,  64.]],\n",
       " \n",
       "        [[ 71.,  71.,  72., ...,  82.,  86.,  77.],\n",
       "         [ 74.,  75.,  76., ...,  87.,  89.,  83.],\n",
       "         [ 71.,  76.,  72., ...,  89.,  93.,  82.],\n",
       "         ...,\n",
       "         [ 79.,  66.,  66., ...,  84.,  86.,  61.],\n",
       "         [ 80.,  76.,  69., ...,  81., 116.,  62.],\n",
       "         [ 93.,  93.,  68., ...,  79., 128.,  67.]],\n",
       " \n",
       "        [[ 94., 106.,  71., ...,  80., 127.,  68.],\n",
       "         [ 97., 111.,  77., ...,  85., 117.,  78.],\n",
       "         [ 79., 104.,  72., ...,  88., 108.,  79.],\n",
       "         ...,\n",
       "         [ 54.,  37.,  42., ...,  50.,  43.,  63.],\n",
       "         [ 51.,  52.,  43., ...,  50.,  52.,  63.],\n",
       "         [ 64.,  65.,  56., ...,  59.,  70.,  70.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 21.,  22.,  15., ...,  20.,  15.,  14.],\n",
       "         [ 25.,  20.,  15., ...,  15.,  15.,  21.],\n",
       "         [ 32.,  34.,  24., ...,  26.,  20.,  26.],\n",
       "         ...,\n",
       "         [102., 104., 118., ..., 122., 126., 106.],\n",
       "         [ 94., 105., 116., ..., 108., 116., 111.],\n",
       "         [ 92., 101., 102., ..., 102., 108.,  99.]],\n",
       " \n",
       "        [[ 79.,  79.,  87., ...,  96.,  88.,  84.],\n",
       "         [ 73.,  80.,  85., ...,  91.,  82.,  92.],\n",
       "         [ 78.,  78.,  73., ...,  80.,  75., 106.],\n",
       "         ...,\n",
       "         [ 23.,  24.,  22., ...,  24.,  23.,  16.],\n",
       "         [ 30.,  27.,  24., ...,  26.,  22.,  23.],\n",
       "         [ 25.,  25.,  20., ...,  27.,  19.,  22.]],\n",
       " \n",
       "        [[ 23.,  20.,  23., ...,  34.,  23.,  18.],\n",
       "         [ 26.,  25.,  23., ...,  19.,  23.,  20.],\n",
       "         [ 24.,  26.,  27., ...,  46.,  28.,  23.],\n",
       "         ...,\n",
       "         [ 42.,  41.,  43., ...,  49.,  44.,  40.],\n",
       "         [ 45.,  41.,  39., ...,  54.,  43.,  44.],\n",
       "         [ 48.,  43.,  45., ...,  53.,  43.,  47.]]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm2_5s_2_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujzaIvbJatpS"
   },
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNNgRCIVatpS"
   },
   "outputs": [],
   "source": [
    "class AirQualityEncodeDataset(Dataset):\n",
    "    def __init__(self, data_e, transform=None):\n",
    "        self.data_e = data_e\n",
    "    def __len__(self):\n",
    "        return self.data_e.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx=idx.tolist()\n",
    "        return self.data_e[idx, :, :]\n",
    "\n",
    "class AirQualityDecodeDataset(Dataset):\n",
    "    def __init__(self, data_d, transform=None):\n",
    "        self.data_d = data_d\n",
    "    def __len__(self):\n",
    "        return self.data_d.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx=idx.tolist()\n",
    "        return self.data_d[idx, :6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TZV8NvFatpW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "#     set_trace()\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlYxea3OatpZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    :param epoch: epoch number\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'encoder': encoder.state_dict(),\n",
    "             'decoder': decoder.state_dict(),\n",
    "             'encoder_optimizer': encoder_optimizer,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = '../checkpoint_lstm.pth.tar'\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train_one_epoch(encode_dataloader, decode_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    hidden = encoder.initHidden()\n",
    "    # cn = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    for ei, sample in enumerate(encode_dataloader):\n",
    "        _, hidden = encoder(sample, hidden)\n",
    "        \n",
    "    \n",
    "#     for ei in range(input_length//256):\n",
    "#         encoder_output, hidden = encoder(\n",
    "#                 input_tensor[ei*256:(ei+1)*256], hidden)\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    first = True\n",
    "    losses = []\n",
    "    for di, sample in enumerate(decode_dataloader):\n",
    "        \n",
    "        decoder_output, hidden = decoder(decoder_input, hidden) \n",
    "        # set_trace()\n",
    "        loss = criterion(decoder_output.view(256, -1), sample.view(256, -1))\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "#         loss += criterion(decoder_output, target_tensor[di, :].long())\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = sample\n",
    "        else:\n",
    "            # decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            # topv, topi = decoder_output.topk(1, dim=2)\n",
    "            # decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = decoder_output\n",
    "    #         print(decoder_output.shape)\n",
    "    #         print(decoder_input.shape)\n",
    "    #         set_trace()\n",
    "    # set_trace()\n",
    "    \n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return np.mean(np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1ZdB_WJatpc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def train(data_e, data_d, hyperparameters, print_every=1, plot_every=1):\n",
    "    \n",
    "    (hidden_size, input_size, output_size, \n",
    "     learning_rate, batch_size) = hyperparameters\n",
    "    \n",
    "    encoder = EncoderRNN(input_size, hidden_size, batch_size).to(device)\n",
    "    decoder = DecoderRNN(input_size, hidden_size, output_size, batch_size).to(device)\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    epoch = 1\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    # criterion = nn.NLLLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "#     g, t, f= year_data.shape\n",
    "    \n",
    "    input_tensor = data_e\n",
    "    input_tensor = torch.from_numpy(input_tensor).to(device)\n",
    "    target_tensor = data_d\n",
    "    target_tensor = torch.from_numpy(target_tensor).to(device)\n",
    "    \n",
    "    set_trace()\n",
    "    encode_dataset = AirQualityEncodeDataset(input_tensor)\n",
    "    decode_dataset = AirQualityDecodeDataset(target_tensor)\n",
    "    set_trace()\n",
    "    encode_dataloader = DataLoader(encode_dataset, \n",
    "                                    batch_size=batch_size)\n",
    "    decode_dataloader = DataLoader(decode_dataset,\n",
    "                                    batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(epoch, epochs+1):\n",
    "        set_trace()\n",
    "        loss = train_one_epoch(encode_dataloader, decode_dataloader, encoder,\n",
    "                 decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epochs),\n",
    "                                         epoch, epoch / epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVlois7hjV_V"
   },
   "outputs": [],
   "source": [
    "# len(pm2_5s_2_years[1:])\n",
    "# print(feature_data.shape)\n",
    "# data_e = feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNx9ZfIWatpf"
   },
   "outputs": [],
   "source": [
    "flag = 1\n",
    "\n",
    "if flag:\n",
    "    data_e = concat_years(feature_data)\n",
    "    data_d = concat_years(pm2_5s_2_years[1:])\n",
    "    # set_trace()\n",
    "    data_d = data_d[:, :6, :]\n",
    "else:\n",
    "    temp = concat_years(pm2_5s_1_years[1:])\n",
    "    data_e = temp[:, :24, :]\n",
    "    data_d = temp[:, 24:24+6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "colab_type": "code",
    "id": "6zXLq4W7atpk",
    "outputId": "11f3e27a-c3e7-47fd-80c4-b34926211e71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training from epoch 1\n",
      "256\n",
      "256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 2.00 GiB total capacity; 1.28 GiB already allocated; 34.43 MiB free; 1.30 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-f14079000564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start training from epoch %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_e\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch trained (%d) exceeds maximum epochs (%d)\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-852286ed761e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_e, data_d, hyperparameters, print_every, plot_every)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_e\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 2.00 GiB total capacity; 1.28 GiB already allocated; 34.43 MiB free; 1.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# class AttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "#         super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.input_size = input_size\n",
    "#         self.seq_len = 6\n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, )\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.out_1 = nn.Linear(1, 6)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "#     def forward(self, input, hn, cn, encoder_outputs):\n",
    "#         output = input.view(self.seq_len, 1, self.input_size)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat(()))\n",
    "#         )\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "#         output = output.view(self.seq_len, self.hidden_size)\n",
    "        \n",
    "#         output = self.out(output).view(self.seq_len, self.output_size, 1)\n",
    "#         output = self.out_1(output)\n",
    "#         output = self.softmax(output)\n",
    "#         output = output.view(-1, 6)\n",
    "# #         set_trace()\n",
    "# #         output = output.view(self.seq_len, self.input_size)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "        \n",
    "#         return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        print(batch_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 1\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = input.view(self.seq_len, -1, self.input_size)\n",
    "        # set_trace()\n",
    "        output = F.relu(output)\n",
    "        # set_trace()\n",
    "        output, hidden = self.gru(output.float(), hidden)\n",
    "        output = output.view(self.seq_len, -1, self.hidden_size)\n",
    "        # set_trace()\n",
    "        output = F.relu(self.out(output))\n",
    "        output = output.view(self.seq_len, -1, self.output_size)\n",
    "        # output = F.relu(self.out_11(output))\n",
    "        # output = F.relu(self.out_12(output))\n",
    "        # output = self.softmax(output)\n",
    "        # output = output.view(-1, 6)\n",
    "#         set_trace()\n",
    "#         output = output.view(self.seq_len, self.input_size)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 24\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        print(batch_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        if hidden.size(1) != self.batch_size:\n",
    "            set_trace()\n",
    "        set_trace()\n",
    "        input = input.to(device)\n",
    "        output = input.view(self.seq_len, -1, self.input_size)\n",
    "        output, hidden = self.gru(output.float(), hidden)\n",
    "        \n",
    "#         set_trace()\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "\n",
    "        return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "input_size = 35\n",
    "# input_size_dec = 35\n",
    "output_size = 35\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "hyperparameters = (hidden_size, input_size,\n",
    "                  output_size, learning_rate, batch_size)\n",
    "\n",
    "checkpoint = '../checkpoint_lstm.pth.tar'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# if os.path.exists(checkpoint):\n",
    "#     checkpoint = torch.load(checkpoint)    \n",
    "#     encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "#     decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "    \n",
    "#     epoch = checkpoint[\"epoch\"]+1\n",
    "#     encoder_optimizer = checkpoint[\"encoder_optimizer\"]\n",
    "#     decoder_optimizer = checkpoint[\"decoder_optimizer\"]\n",
    "# else:\n",
    "epoch = 1\n",
    "\n",
    "\n",
    "if epoch <= epochs:\n",
    "    print(\"start training from epoch %d\" %epoch)\n",
    "    train(data_e, data_d, hyperparameters)\n",
    "else:\n",
    "    print(\"epoch trained (%d) exceeds maximum epochs (%d)\" %(epoch, epochs))\n",
    "print(\"finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qy8oP0ikkR7E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGwzYgw5atpn"
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwZL_euatpo"
   },
   "outputs": [],
   "source": [
    "def normalize_data_pm2_5(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "def accuracy(pred, target):\n",
    "#     set_trace()\n",
    "    pred = pred.squeeze()\n",
    "    np_pred = pred.cpu().numpy()\n",
    "    np_target = target.cpu().numpy()\n",
    "    d, k = pred.shape\n",
    "    accu = np.sum(np_pred==np_target)/(d*k)\n",
    "    return accu\n",
    "\n",
    "def evaluate(input_tensor, target_tensor, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    accu_all = []\n",
    "    \n",
    "    loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = encoder.initHidden()\n",
    "        \n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "    #     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, hidden = encoder(\n",
    "                input_tensor[ei], hidden)\n",
    "\n",
    "\n",
    "        decoder_input = target_tensor[0, :]\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, hidden = decoder(decoder_input, hidden) \n",
    "            loss += criterion(decoder_output, target_tensor[di, :].view(-1,).long())\n",
    "\n",
    "            decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            topv, topi = decoder_output.topk(1, dim=2)\n",
    "            \n",
    "            accu_all.append(accuracy(topi, target_tensor[di, :]))\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "#     set_trace()\n",
    "    return loss.item() / target_length, np.mean(accu_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QG5vJe9atpr"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.from_numpy(pm2_5s_1_years[0][:,:24,:]).to(device)\n",
    "target_tensor = torch.from_numpy(pm2_5s_1_years[0][:,24:24+6,:]).to(device)\n",
    "eval_loss, eval_accu = evaluate(input_tensor, target_tensor, encoder, decoder)\n",
    "print(\"eval_loss: \", eval_loss)\n",
    "print(\"eval_accu: \", eval_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2uy4zP3atpu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVVG1-HUatpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_pm2_5_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
