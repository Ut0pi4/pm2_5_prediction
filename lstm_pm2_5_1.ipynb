{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "L77Rq4wedYYz",
    "outputId": "b35c5b47-08e5-4eab-dc41-fc037a28c7da"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# if os.path.exists(\"../air_quality\"):\n",
    "#    !rm -rf \"../air_quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaqvGroEatoZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "\n",
    "from pdb import set_trace\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SOURCE_URL = \"https://cloud.tsinghua.edu.cn/d/f519a587a6d943fa9aa0/files/?p=%2F%E5%8C%97%E4%BA%AC%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F.zip&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cU7gERKaatoh"
   },
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5e8dIEexatoi",
    "outputId": "7d0b506c-46d3-431f-d821-d3c2ae33ef91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 北京空气质量.zip\n",
      "Extracting ../air_quality\\北京空气质量\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, work_directory):\n",
    "    \"\"\"Download the data from website, unless it's already here.\"\"\"\n",
    "    if not tf.io.gfile.exists(work_directory):\n",
    "        tf.io.gfile.makedirs(work_directory)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    # set_trace()\n",
    "    if not tf.io.gfile.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath)\n",
    "    with tf.io.gfile.GFile(filepath) as f:\n",
    "        print('Successfully downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "def extract_files(filepath):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    # filepath = Path(filepath)\n",
    "    print('Extracting', filepath)\n",
    "    #if not tf.io.gfile.exists(filepath):\n",
    "    #    tf.io.gfile.makedirs(filepath)\n",
    "        \n",
    "    with zipfile.ZipFile(filepath+\".zip\", 'r') as zip_ref:\n",
    "        #zip_ref.extractall(\"../air_quality/\")\n",
    "        \n",
    "        for member in zip_ref.infolist():\n",
    "            # set_trace()\n",
    "            \n",
    "            member.filename = member.filename.encode(\"cp437\").decode(\"utf8\")\n",
    "\n",
    "            zip_ref.extract(member, \"../air_quality/\")\n",
    "#             set_trace()\n",
    "# #             if member.filename == \"北京空气质量\"：\n",
    "# #                 os.rename(member.filename, \"pm_2_5_data\")\n",
    "#             set_trace()\n",
    "    #os.chdir(filepath) # change directory from working dir to dir with files\n",
    "\n",
    "    for item in os.listdir(filepath): # loop through items in dir\n",
    "        #set_trace()\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            #file_name = os.path.abspath(item) # get full path of files\n",
    "            zipfile_path = os.path.join(filepath, item)\n",
    "            zip_ref = zipfile.ZipFile(zipfile_path) # create zipfile object\n",
    "            zip_ref.extractall(filepath) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name) # delete zipped file\n",
    "\n",
    "def download_and_extract():\n",
    "    dest = \"../air_quality\"\n",
    "    file_name = \"北京空气质量.zip\"\n",
    "    filepath = maybe_download(file_name, dest)\n",
    "    filepath = filepath[:-4]\n",
    "    extract_files(filepath)\n",
    "download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEqoqRihatom"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data_pm2_5(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "def process_data(data, pm2_5=0):\n",
    "#     data = list(map(int, data))\n",
    "    N, d = data.shape\n",
    "    \n",
    "    new_data = np.zeros((N, d))\n",
    "#     new_data = []\n",
    "   \n",
    "    m = []\n",
    "    for j, row in enumerate(data):\n",
    "        row = [float(i) if i != \"\" else 0 for i in row]\n",
    "        row = np.array(row)\n",
    "       \n",
    "        if np.any(row==0):\n",
    "#             set_trace()\n",
    "            index = np.where(row==0)\n",
    "#             set_trace()\n",
    "            if len(index[0]) == len(row):\n",
    "                m.append(j)\n",
    "                continue\n",
    "            #for ind in index[0]:\n",
    "                \n",
    "            mu = np.sum(row)/(len(row)-len(index[0]))\n",
    "            row[index[0]] = mu\n",
    "#             set_trace()\n",
    "            if mu <= 0:\n",
    "                set_trace()\n",
    "#         set_trace()\n",
    "        new_data[j, :] = row\n",
    "        \n",
    "        \n",
    "    if m:\n",
    "        for ind in m:    \n",
    "            new_data[ind] = np.sum(new_data, axis=0)/(len(new_data)-len(m))\n",
    "    \n",
    "    if pm2_5:\n",
    "        new_data = normalize_data_pm2_5(new_data)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "def check_data(row, item, year):\n",
    "    check = [True if i==\"\" else False for i in row[3:]]\n",
    "    check_true = True\n",
    "    if np.all(np.array(check)):\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "           \n",
    "            check_true = False\n",
    "    return check_true\n",
    "\n",
    "def add_removed(item, year):\n",
    "    split = item.split(\"_\")\n",
    "    split[-2] = \"extra\"\n",
    "    item_1 = \"_\"\n",
    "    item_1 = item_1.join(split)\n",
    "    REMOVED_CSV[year].add(item_1)\n",
    "    split[-2] = \"all\"\n",
    "    item_2 = \"_\"\n",
    "    item_2 = item_2.join(split)\n",
    "    REMOVED_CSV[year].add(item_2)\n",
    "    \n",
    "\n",
    "def restructure_data(data):\n",
    "    num_doc = len(data)\n",
    "\n",
    "    N, d = data[0].shape\n",
    "    \n",
    "    new_data = np.zeros((num_doc, N, d))\n",
    "    \n",
    "    for i, dat in enumerate(data):\n",
    "        new_data[i, :, :] = dat\n",
    "    return new_data\n",
    "\n",
    "def clean(folderpath, consistent=False):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        \n",
    "        split = item.split(\"_\")\n",
    "        if \"beijing\" not in item:\n",
    "            REMOVED_CSV[year].add(item)\n",
    "            continue\n",
    "#         set_trace()\n",
    "        if consistent:\n",
    "            if split[-2] == \"extra\":\n",
    "                split_1 = split\n",
    "                split_1[-2] = \"all\"\n",
    "                item_1 = \"_\"\n",
    "                item_1 = item_1.join(split_1)\n",
    "\n",
    "                if item_1 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "            else:\n",
    "                split_2 = split\n",
    "                split_2[-2] = \"extra\"\n",
    "                item_2 = \"_\"\n",
    "                item_2 = item_2.join(split_2)\n",
    "                if item_2 not in os.listdir(folderpath):\n",
    "#                     set_trace()\n",
    "                    REMOVED_CSV[year].add(item)\n",
    "        \n",
    "        \n",
    "        if item in REMOVED_CSV[year]:\n",
    "            continue\n",
    "        \n",
    "        filepath_csv = folderpath + \"/\" + item\n",
    "#         print(filepath_csv)\n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv, encoding=\"utf8\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    #         set_trace()\n",
    "            prev_header = []\n",
    "            for i, row in enumerate(csv_reader):\n",
    "#                 set_trace()\n",
    "                try:\n",
    "                    row[2]==\"SO2\"\n",
    "                        \n",
    "                except:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "                \n",
    "                if len(row[3:]) != 35:\n",
    "                    if item not in REMOVED_CSV[year]:\n",
    "                        add_removed(item, year)\n",
    "#                         REMOVED_CSV[year].add(item)\n",
    "                    break\n",
    "\n",
    "               \n",
    "                    \n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    if prev_header:\n",
    "                        if header != prev_header:\n",
    "                            set_trace()\n",
    "                            break\n",
    "                    prev_header = header\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "#                             REMOVED_CSV[year].add(item)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "\n",
    "        split = filepath_csv.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            if len(pm2_5) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "        elif split[-2] == \"extra\":\n",
    "            if len(SO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(NO2) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(CO) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "            if len(O3) != 24:\n",
    "                add_removed(item, year)\n",
    "                continue\n",
    "    print(\"complete read folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "BMU7gIg9atoq",
    "outputId": "18f2fa5e-5b02-41e1-c434-13418818e466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of REMOVED_CSV in year 2014: 2\n",
      "length of REMOVED_CSV in year 2015: 0\n",
      "length of REMOVED_CSV in year 2016: 0\n",
      "length of REMOVED_CSV in year 2017: 0\n",
      "length of REMOVED_CSV in year 2018: 0\n",
      "length of REMOVED_CSV in year 2019: 0\n",
      "length of REMOVED_CSV in year 2020: 0\n",
      "\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "length of REMOVED_CSV in year 2014: 83\n",
      "length of REMOVED_CSV in year 2015: 29\n",
      "length of REMOVED_CSV in year 2016: 64\n",
      "length of REMOVED_CSV in year 2017: 240\n",
      "length of REMOVED_CSV in year 2018: 118\n",
      "length of REMOVED_CSV in year 2019: 116\n",
      "length of REMOVED_CSV in year 2020: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REMOVED_CSV = {}\n",
    "REMOVED_CSV[\"2014\"] = {\"beijing_all_20141231.csv\", \"beijing_extra_20141231.csv\"}\n",
    "REMOVED_CSV[\"2015\"] = set()\n",
    "REMOVED_CSV[\"2016\"] = set()\n",
    "REMOVED_CSV[\"2017\"] = set()\n",
    "REMOVED_CSV[\"2018\"] = set()\n",
    "REMOVED_CSV[\"2019\"] = set()\n",
    "REMOVED_CSV[\"2020\"] = set()\n",
    "\n",
    "folderpath = \"../air_quality/北京空气质量\"\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")  \n",
    "\n",
    "\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            clean(folderpath+\"/\"+item)\n",
    "\n",
    "for key, values in REMOVED_CSV.items():\n",
    "    print(\"length of REMOVED_CSV in year %s: %d\" %(key, len(values)))\n",
    "print(\"\")            \n",
    "\n",
    "# print(\"Start checking REMOVED_CSV pairs\")\n",
    "# for key, values in REMOVED_CSV.items():\n",
    "#     for value_a in values:\n",
    "#         if \"beijing\" in value_a:\n",
    "#             two = 1\n",
    "#             split = value_a.split(\"_\")\n",
    "#             for value_b in values:\n",
    "#                 if value_a != value_b:\n",
    "#                     if split[-1] in value_b:\n",
    "#                         set_trace()\n",
    "#                         two += 1\n",
    "#                         break                    \n",
    "#             if two != 2:\n",
    "#                 set_trace()\n",
    "# print(\"Complete check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "x-nRRfjLatov",
    "outputId": "3bdf47d1-c4d4-4d41-d4c9-af7caf26dad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20140101-20141231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20150101-20151231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20160101-20161231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20170101-20171231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20180101-20181231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20190101-20191231\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n",
      "start reading csv in folder:  ../air_quality/北京空气质量/beijing_20200101-20200509\n",
      "complete read folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def retrieve_data_alls(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    # set_trace()\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "            split = item.split(\"_\")\n",
    "            if split[-2] == \"extra\":\n",
    "                extras.append(item)\n",
    "            else:\n",
    "                alls.append(item)\n",
    "    alls.sort()\n",
    "    extras.sort()\n",
    "    alls_pairs = []\n",
    "    # set_trace()\n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = alls[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "\n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            alls_pairs.append((alls[i], alls[i+1]))\n",
    "    # set_trace()\n",
    "    for (item_1, item_2) in alls_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        pm2_5 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "            \n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 1)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s\n",
    "\n",
    "def retrieve_data_extras(folderpath):\n",
    "\n",
    "    print(\"start reading csv in folder: \", folderpath)\n",
    "    pm2_5s = []\n",
    "    SO2s = []\n",
    "    NO2s = []\n",
    "    COs = []\n",
    "    O3s = []\n",
    "    \n",
    "    year = folderpath.split(\"-\")[1][:4]\n",
    "    items = []\n",
    "    alls = []\n",
    "    extras = []\n",
    "    \n",
    "    for item in os.listdir(folderpath):\n",
    "        if \"beijing\" not in item:\n",
    "            continue\n",
    "        if item not in REMOVED_CSV[year]:\n",
    "            items.append(item)\n",
    "\n",
    "    for item in items:\n",
    "        split = item.split(\"_\")\n",
    "        if split[-2] == \"all\":\n",
    "            split_1 = split\n",
    "            split_1[-2] = \"extra\"\n",
    "            item_1 = \"_\"\n",
    "            item_1 = item_1.join(split_1)\n",
    "            if item_1 in items:\n",
    "                extras.append(item_1)\n",
    "                alls.append(item)\n",
    "    \n",
    "    alls.sort()\n",
    "    extras.sort()\n",
    "    assert(len(extras)==len(alls))\n",
    "    extra_all_pairs = []\n",
    "    \n",
    "    for i in range(len(alls)-1):\n",
    "        split_1 = extras[i].split(\"_\")\n",
    "        num_csv_1 = (int)(split_1[-1][:8])\n",
    "        split_2 = alls[i+1].split(\"_\")\n",
    "        num_csv_2 = (int)(split_2[-1][:8])\n",
    "        \n",
    "        if num_csv_2 - 1 == num_csv_1:\n",
    "            extra_all_pairs.append((extras[i], alls[i+1]))\n",
    "        \n",
    "#     set_trace()\n",
    "    for (item_1, item_2) in extra_all_pairs:\n",
    "\n",
    "        \n",
    "        filepath_csv_1 = folderpath + \"/\" + item_1\n",
    "        filepath_csv_2 = folderpath + \"/\" + item_2\n",
    "        \n",
    "        \n",
    "        pm2_5 = []\n",
    "        SO2 = []\n",
    "        NO2 = []\n",
    "        CO = []\n",
    "        O3 = []\n",
    "        d = 0\n",
    "        \n",
    "        with open(filepath_csv_1, encoding=\"utf8\") as csv_file_1:\n",
    "           \n",
    "        \n",
    "            csv_reader_1 = csv.reader(csv_file_1, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_1):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "        with open(filepath_csv_2, encoding=\"utf8\") as csv_file_2:\n",
    "            csv_reader_2 = csv.reader(csv_file_2, delimiter=\",\")\n",
    "\n",
    "            for i, row in enumerate(csv_reader_2):\n",
    "\n",
    "                if i==0:\n",
    "                    header = row[3:]\n",
    "                    continue\n",
    "                else:\n",
    "                    if row[2] in [\"PM2.5\", \"SO2\", \"NO2\", \"CO\", \"O3\"]:\n",
    "                        if not check_data(row[3:], item, year):\n",
    "                            add_removed(item, year)\n",
    "                            break\n",
    "                        else:\n",
    "                            if row[2]==\"PM2.5\":\n",
    "                                pm2_5.append(row[3:])\n",
    "                            elif row[2]==\"SO2\":\n",
    "                                SO2.append(row[3:])\n",
    "                            elif row[2]==\"NO2\":\n",
    "                                NO2.append(row[3:])\n",
    "                            elif row[2]==\"CO\":\n",
    "                                CO.append(row[3:])\n",
    "                            elif row[2]==\"O3\":\n",
    "                                O3.append(row[3:])\n",
    "                        \n",
    "\n",
    "        if pm2_5:\n",
    "            pm2_5 = np.array(pm2_5)\n",
    "            pm2_5 = process_data(pm2_5, 1)\n",
    "            pm2_5s.append(pm2_5)\n",
    "\n",
    "        if SO2:\n",
    "            SO2 = np.array(SO2)\n",
    "#             set_trace()\n",
    "            SO2 = process_data(SO2)\n",
    "#             set_trace()\n",
    "            SO2s.append(SO2)\n",
    "\n",
    "        if NO2:\n",
    "            NO2 = np.array(NO2)\n",
    "            NO2 = process_data(NO2)\n",
    "            NO2s.append(NO2)\n",
    "\n",
    "        if CO:\n",
    "            CO = np.array(CO)\n",
    "            CO = process_data(CO)\n",
    "            COs.append(CO)\n",
    "\n",
    "        if O3:\n",
    "            O3 = np.array(O3)\n",
    "            O3 = process_data(O3)\n",
    "            O3s.append(O3)\n",
    "    \n",
    "    pm2_5s = restructure_data(pm2_5s)\n",
    "    SO2s = restructure_data(SO2s)\n",
    "    NO2s = restructure_data(NO2s)\n",
    "    COs = restructure_data(COs)\n",
    "    O3s = restructure_data(O3s)\n",
    "    \n",
    "    print(\"complete read folder\")\n",
    "    return pm2_5s, SO2s, NO2s, COs, O3s\n",
    "#     return pm2_5s\n",
    "\n",
    "pm2_5s_1_years = []\n",
    "pm2_5s_2_years = []\n",
    "SO2s_years = []\n",
    "NO2s_years = []\n",
    "COs_years = []\n",
    "O3s_years = []\n",
    "if tf.io.gfile.exists(folderpath):\n",
    "    for item in os.listdir(folderpath):\n",
    "        if \".zip\" not in item and \"beijing\" in item:\n",
    "            pm2_5s_1 = retrieve_data_alls(folderpath+\"/\"+item)\n",
    "            pm2_5s_1_years.append(pm2_5s_1)\n",
    "            \n",
    "            pm2_5s_2, SO2s, NO2s, COs, O3s = retrieve_data_extras(folderpath+\"/\"+item)  \n",
    "            pm2_5s_2_years.append(pm2_5s_2)\n",
    "            SO2s_years.append(SO2s)\n",
    "            NO2s_years.append(NO2s)\n",
    "            COs_years.append(COs)\n",
    "            O3s_years.append(O3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gf04_Ltkatoz",
    "outputId": "ac0d500f-b24e-48ad-cb3f-c47495acfd19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[46.        , 46.        , 48.        , ..., 30.        ,\n",
       "         19.        , 11.        ],\n",
       "        [41.        , 40.        , 44.        , ..., 27.        ,\n",
       "         36.79411765, 31.        ],\n",
       "        [29.        , 32.        , 35.        , ..., 25.        ,\n",
       "         32.09090909,  2.        ],\n",
       "        ...,\n",
       "        [50.        , 32.        , 47.        , ..., 18.        ,\n",
       "          2.        ,  3.        ],\n",
       "        [34.        , 20.        , 26.        , ...,  3.        ,\n",
       "         24.08823529,  2.        ],\n",
       "        [30.        , 33.        , 12.        , ...,  2.        ,\n",
       "         13.        , 10.        ]],\n",
       "\n",
       "       [[44.        , 52.        , 34.        , ..., 20.        ,\n",
       "         27.        , 31.        ],\n",
       "        [45.        , 48.        , 44.        , ..., 30.        ,\n",
       "         33.78787879, 40.        ],\n",
       "        [51.        , 57.        , 55.        , ..., 39.        ,\n",
       "         28.        , 47.        ],\n",
       "        ...,\n",
       "        [78.        , 69.        , 84.        , ..., 64.        ,\n",
       "         35.        , 30.        ],\n",
       "        [63.        , 71.        , 73.        , ..., 57.        ,\n",
       "         79.70588235, 29.        ],\n",
       "        [65.        , 66.        , 62.        , ..., 39.        ,\n",
       "         37.        , 35.        ]],\n",
       "\n",
       "       [[22.        , 11.        , 10.        , ..., 10.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.        ,  7.        ,  2.        , ...,  2.        ,\n",
       "         18.12121212,  2.        ],\n",
       "        [ 3.        ,  4.        ,  2.        , ...,  2.        ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [52.        , 63.        , 86.        , ..., 33.        ,\n",
       "          8.        ,  3.        ],\n",
       "        [ 3.        , 18.        , 67.        , ..., 22.        ,\n",
       "          2.        ,  2.        ],\n",
       "        [11.        ,  5.        , 29.        , ...,  7.        ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.5       ,  8.        ,  2.        , ...,  3.7       ,\n",
       "          2.        ,  6.7       ],\n",
       "        [ 2.7       ,  7.8       ,  2.        , ...,  2.        ,\n",
       "          2.        ,  7.        ],\n",
       "        [ 5.1       ,  7.6       ,  2.        , ...,  2.7       ,\n",
       "          2.        ,  5.2       ],\n",
       "        ...,\n",
       "        [ 2.        ,  9.1       ,  2.        , ...,  5.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  8.5       ,  2.1       , ...,  4.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 2.        ,  9.        ,  2.7       , ...,  4.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[10.371875  , 12.1       ,  6.6       , ...,  3.3       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 9.26875   , 11.7       ,  6.        , ...,  2.9       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 8.628125  , 11.2       ,  5.8       , ...,  2.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 6.        , 12.8       ,  7.9       , ...,  6.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.8       ,  8.2       , ...,  5.8       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 12.5       ,  7.5       , ...,  5.6       ,\n",
       "          2.        ,  2.        ]],\n",
       "\n",
       "       [[ 6.5       , 12.5       ,  7.9       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 13.3       ,  7.        , ...,  6.1       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.8       , 13.4       ,  6.2       , ...,  5.7       ,\n",
       "          2.        ,  2.        ],\n",
       "        ...,\n",
       "        [ 5.7       , 11.2       ,  6.9       , ...,  4.2       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 5.9       , 11.5       ,  7.        , ...,  3.4       ,\n",
       "          2.        ,  2.        ],\n",
       "        [ 6.2       , 12.4       ,  7.3       , ...,  2.3       ,\n",
       "          2.        ,  2.        ]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pm2_5s_2_years[0]\n",
    "O3s_years[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wn44ZrQiato4"
   },
   "outputs": [],
   "source": [
    "def normalize_concat(arr_data):\n",
    "    norm_arr_data = []\n",
    "    for data in arr_data:\n",
    "        norm_arr_data.append(normalize(data))\n",
    "#     set_trace()\n",
    "    return concat(norm_arr_data)\n",
    "\n",
    "def normalize(data):\n",
    "    new_data = []\n",
    "    for dat in data:\n",
    "#         set_trace()\n",
    "        temp = (dat-np.min(dat))/(np.max(dat)-np.min(dat))\n",
    "        new_data.append(temp)\n",
    "    \n",
    "#     set_trace()\n",
    "    return new_data\n",
    "\n",
    "def concat(arr_data):\n",
    "    years = len(arr_data[0])\n",
    "    new_data = []\n",
    "    for year in range(years):\n",
    "        temp = []\n",
    "        for data in arr_data:\n",
    "            temp.append(data[year])\n",
    "        \n",
    "        new_data.append(np.concatenate(temp, 0))\n",
    "#         set_trace()\n",
    "    return new_data\n",
    "\n",
    "feature_data = normalize_concat((SO2s_years, NO2s_years, O3s_years, COs_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "-zwKSHjZato8",
    "outputId": "e3255268-6726-402c-bbbf-884488dea97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years: 7\n",
      "data for 2015: (836, 24, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"years:\", len(feature_data))\n",
    "print(\"data for 2015:\", feature_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mN6QexKcatpA"
   },
   "outputs": [],
   "source": [
    "def concat_years(arr_data):\n",
    "    years = len(arr_data)\n",
    "    sum_doc = 0\n",
    "    N, h, f = arr_data[0].shape\n",
    "    for year in range(years):\n",
    "        sum_doc += arr_data[year].shape[0]\n",
    "    new_data = np.zeros((sum_doc, h, f))\n",
    "    ind = 0\n",
    "    for year in range(years):\n",
    "        ind_last = arr_data[year].shape[0]\n",
    "        new_data[ind:ind+ind_last, :, :] = arr_data[year]\n",
    "        ind = ind + ind_last\n",
    "    return new_data    \n",
    "# train_data = concat_years(pm2_5s_1_years[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NleZ4Q5atpE"
   },
   "source": [
    "Training Model: seq2seq <br>\n",
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6tnvW5ratpF"
   },
   "outputs": [],
   "source": [
    "\n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "# #         embedded = self.embedding(input).view(1, 1, -1)\n",
    "# #         output = embedded\n",
    "#         output = input.view(24,1,6)\n",
    "# #         set_trace()\n",
    "#         output, hidden = self.lstm(output.float(), (hn.detach(), cn.detach()))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "# #         h_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         c_hidden = torch.zeros(1, 1, self.hidden_size)\n",
    "# #         if torch.cuda.is_available():\n",
    "# #             return h_hidden.cuda(), c_hidden.cuda()\n",
    "# #         else:\n",
    "# #             return h_hidden, c_hidden\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-l_gd9_LatpI"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwbjJRVNatpJ"
   },
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hn, cn):\n",
    "#         output = input.view(24, self.hidden_size)\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.lstm(output, (hn, cn))\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "        \n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVBDlT4catpO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujzaIvbJatpS"
   },
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNNgRCIVatpS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train_one_epoch(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    hn = encoder.initHidden()\n",
    "    cn = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (hn, cn) = encoder(\n",
    "            input_tensor[ei], hn, cn)\n",
    "        \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    decoder_input = target_tensor[0, :]\n",
    "           \n",
    "    for di in range(target_length):\n",
    "#         set_trace()\n",
    "        decoder_output, (hn, cn) = decoder(decoder_input, hn, cn) \n",
    "        loss += criterion(decoder_output, target_tensor[di, :].view(-1,).long())\n",
    "#         loss += criterion(decoder_output, target_tensor[di, :].long())\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di, :]\n",
    "        else:\n",
    "            decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            topv, topi = decoder_output.topk(1, dim=2)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "#             decoder_input = decoder_output\n",
    "#         print(decoder_output.shape)\n",
    "#         print(decoder_input.shape)\n",
    "#         set_trace()\n",
    "    set_trace()\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TZV8NvFatpW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "#     set_trace()\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlYxea3OatpZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1ZdB_WJatpc"
   },
   "outputs": [],
   "source": [
    "def reshape_data(data, i):\n",
    "    data1 = data[:, :, :i]\n",
    "    data2 = data[:, :, i+1:]\n",
    "    new_data = np.concatenate((data1, data2), axis=2)\n",
    "#     set_trace()\n",
    "#     new_data = new_data.reshape((-1, 24, 34))\n",
    "    return new_data\n",
    "\n",
    "def save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    :param epoch: epoch number\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'encoder': encoder.state_dict(),\n",
    "             'decoder': decoder.state_dict(),\n",
    "             'encoder_optimizer': encoder_optimizer,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = '../checkpoint_lstm.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "\n",
    "    \n",
    "def train(data_e, data_d, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "               start_epoch, epochs, print_every=1, plot_every=1, learning_rate=0.01):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "#     criterion = nn.MSELoss()\n",
    "#     g, t, f= year_data.shape\n",
    "    \n",
    "    input_tensor = data_e\n",
    "    input_tensor = torch.from_numpy(input_tensor).to(device)\n",
    "    target_tensor = data_d\n",
    "    target_tensor = torch.from_numpy(target_tensor).to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "\n",
    "        loss = train_one_epoch(input_tensor, target_tensor, encoder,\n",
    "                 decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epochs),\n",
    "                                         epoch, epoch / epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BVlois7hjV_V",
    "outputId": "9989290b-898c-4f2f-ecd9-a1ef444bff9e"
   },
   "outputs": [],
   "source": [
    "# len(pm2_5s_2_years[1:])\n",
    "# print(feature_data.shape)\n",
    "# data_e = feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNx9ZfIWatpf"
   },
   "outputs": [],
   "source": [
    "flag = 1\n",
    "\n",
    "if flag:\n",
    "    data_e = concat_years(feature_data)\n",
    "    data_d = concat_years(pm2_5s_2_years[1:])\n",
    "    # set_trace()\n",
    "    data_d = data_d[:, :6, :]\n",
    "else:\n",
    "    temp = concat_years(pm2_5s_1_years[1:])\n",
    "    data_e = temp[:, :24, :]\n",
    "    data_d = temp[:, 24:24+6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "input_size_enc = 35\n",
    "input_size_dec = 35\n",
    "output_size = 35\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "checkpoint = '../checkpoint_lstm.pth.tar'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "6zXLq4W7atpk",
    "outputId": "03ea598d-0bde-44a0-94ee-470cd8be4f73"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# class AttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "#         super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.input_size = input_size\n",
    "#         self.seq_len = 6\n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, )\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.out_1 = nn.Linear(1, 6)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "#     def forward(self, input, hn, cn, encoder_outputs):\n",
    "#         output = input.view(self.seq_len, 1, self.input_size)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat(()))\n",
    "#         )\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "#         output = output.view(self.seq_len, self.hidden_size)\n",
    "        \n",
    "#         output = self.out(output).view(self.seq_len, self.output_size, 1)\n",
    "#         output = self.out_1(output)\n",
    "#         output = self.softmax(output)\n",
    "#         output = output.view(-1, 6)\n",
    "# #         set_trace()\n",
    "# #         output = output.view(self.seq_len, self.input_size)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "        \n",
    "#         return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 6\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.out_11 = nn.Linear(1, hidden_size)\n",
    "        self.out_12 = nn.Linear(hidden_size, 6)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "        output = input.view(self.seq_len, 1, self.input_size)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "        output = output.view(self.seq_len, self.hidden_size)\n",
    "        \n",
    "        output = self.out(output).view(self.seq_len, self.output_size, 1)\n",
    "        output = F.relu(self.out_11(output))\n",
    "        output = F.relu(self.out_12(output))\n",
    "        output = self.softmax(output)\n",
    "        output = output.view(-1, 6)\n",
    "#         set_trace()\n",
    "#         output = output.view(self.seq_len, self.input_size)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.seq_len = 24\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hn, cn):\n",
    "        \n",
    "        output = input.view(self.seq_len, 1, self.input_size)\n",
    "        output, hidden = self.lstm(output.float(), (hn, cn))\n",
    "        \n",
    "#         set_trace()\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "input_size_enc = 35\n",
    "input_size_dec = 35\n",
    "output_size = 35\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "checkpoint = '../checkpoint_lstm.pth.tar'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "encoder = EncoderRNN(input_size_enc, hidden_size).to(device)\n",
    "decoder = DecoderRNN(input_size_dec, hidden_size, output_size).to(device)\n",
    "\n",
    "# if os.path.exists(checkpoint):\n",
    "#     checkpoint = torch.load(checkpoint)    \n",
    "#     encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "#     decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "    \n",
    "#     epoch = checkpoint[\"epoch\"]+1\n",
    "#     encoder_optimizer = checkpoint[\"encoder_optimizer\"]\n",
    "#     decoder_optimizer = checkpoint[\"decoder_optimizer\"]\n",
    "# else:\n",
    "epoch = 1\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "if epoch <= epochs:\n",
    "    print(\"start training from epoch %d\" %epoch)\n",
    "    train(data_e, data_d, encoder, decoder, encoder_optimizer, decoder_optimizer, epoch, epochs)\n",
    "else:\n",
    "    print(\"epoch trained (%d) exceeds maximum epochs (%d)\" %(epoch, epochs))\n",
    "print(\"finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qy8oP0ikkR7E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGwzYgw5atpn"
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwZL_euatpo"
   },
   "outputs": [],
   "source": [
    "def normalize_data_pm2_5(data):\n",
    "    data_new = copy.deepcopy(data)\n",
    "#     set_trace()\n",
    "    data_new[data_new > 250] = -5\n",
    "    data_new[data_new > 150] = -4\n",
    "    data_new[data_new > 115] = -3\n",
    "    data_new[data_new > 75] = -2\n",
    "    data_new[data_new > 35] = -1\n",
    "    data_new[data_new > 0] = 0\n",
    "    data_new = abs(data_new).astype(int)\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "def accuracy(pred, target):\n",
    "#     set_trace()\n",
    "    pred = pred.squeeze()\n",
    "    np_pred = pred.cpu().numpy()\n",
    "    np_target = target.cpu().numpy()\n",
    "    d, k = pred.shape\n",
    "    accu = np.sum(np_pred==np_target)/(d*k)\n",
    "    if (target ==0).any():\n",
    "        set_trace()\n",
    "    return accu\n",
    "\n",
    "def evaluate(input_tensor, target_tensor, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    accu_all = []\n",
    "    \n",
    "    loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = encoder.initHidden()\n",
    "        \n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "    #     encoder_outputs = torch.zeros(input_length, 24, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, hidden = encoder(\n",
    "                input_tensor[ei], hidden)\n",
    "\n",
    "\n",
    "        decoder_input = target_tensor[0, :]\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, hidden = decoder(decoder_input, hidden) \n",
    "            loss += criterion(decoder_output, target_tensor[di, :].view(-1,).long())\n",
    "\n",
    "            decoder_output = decoder_output.view(6, 35 ,6)\n",
    "            topv, topi = decoder_output.topk(1, dim=2)\n",
    "            \n",
    "            accu_all.append(accuracy(topi, target_tensor[di, :]))\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "#     set_trace()\n",
    "    return loss.item() / target_length, np.mean(accu_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "0QG5vJe9atpr",
    "outputId": "890d75db-c07b-416e-f9cf-ce3ec89c474e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-984349adee0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_e\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_accu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eval_loss: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eval_accu: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_accu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.from_numpy(data_e).to(device)\n",
    "target_tensor = torch.from_numpy(data_d).to(device)\n",
    "eval_loss, eval_accu = evaluate(input_tensor, target_tensor, encoder, decoder)\n",
    "print(\"eval_loss: \", eval_loss)\n",
    "print(\"eval_accu: \", eval_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2uy4zP3atpu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVVG1-HUatpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lstm_pm2_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
